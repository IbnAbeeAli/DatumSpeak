{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY_']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data from a pdf file from simple directory reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"data/Mind2Web.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing inside the structure of document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "24\n",
      "<class 'llama_index.core.schema.Document'>\n",
      "3463\n",
      "Doc ID: 471ce9c8-43c5-4c4d-84fe-aef4ac21516c\n",
      "Text: (a) Find one-way flights from New York to  Toronto. (b) Book a\n",
      "roundtrip on July 1 from Mumbai to  London and vice versa on July 5\n",
      "for two adults.  (c) Find a flight from Chicago to London on  20 April\n",
      "and return on 23 April.  (d) Find Elon Musk's profile and follow,\n",
      "start  notifications and like the latest tweet.  (e) Browse comedy\n",
      "films stream...\n"
     ]
    }
   ],
   "source": [
    "print(type(documents))\n",
    "print(len(documents))\n",
    "print(type(documents[0]))\n",
    "print(len(documents[1].text))\n",
    "print(documents[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll merge it into a single document as it helps with overall accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [doc.text for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '\\n\\n'.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document(text='\\n\\n'.join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: d8141fb3-ab68-474a-9a84-2b5e334b1978\n",
      "Text: MIND2W EB: Towards a Generalist Agent for the Web Xiang Dengâˆ—Yu\n",
      "Gu Boyuan Zheng Shijie Chen Samuel Stevens Boshi Wang Huan Sunâˆ—Yu Suâˆ—\n",
      "The Ohio State University https://osu-nlp-group.github.io/Mind2Web\n",
      "Abstract We introduce MIND2W EB, the first dataset for developing and\n",
      "evaluating generalist agents for the web that can follow language\n",
      "instructio...\n"
     ]
    }
   ],
   "source": [
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing the Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Service Context contains both the LLM and Embedding Model that we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.settings import Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "embed_model = OpenAIEmbedding(model='text-embedding-ada-002')\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "index = VectorStoreIndex.from_documents([document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Case of Showing How to generate embeddings from embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embed_model.get_text_embedding(\"Hello How are you\")\n",
    "embedding2 = embed_model.get_text_embedding(\"Hey What's up\")\n",
    "embedding3 = embed_model.get_text_embedding(\"Why are you here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9046455973572882\n",
      "0.8097210593940989\n",
      "0.805423429745634\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(embedding1, embedding2))\n",
    "print(np.dot(embedding1, embedding3))\n",
    "print(np.dot(embedding3, embedding2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query and Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a query engine from this index that does allows us to send queries that do reterival and synthesis against this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on unseen websites is 38.9% / 39.6%.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What is the figure of their accuracy on unseen websites?\"\n",
    ")\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence-Window Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentenceWindowNodeParser object that split doc into chunks and augment each chunk with surrounding context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key='window',\n",
    "    original_text_metadata_key='original_text'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'hello. How are you? I am fine! Foo. Bar. Baz. What? Nyan! Ni ji chi san. Nyan. Arigato'\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents([Document(text=text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello. ', 'How are you? ', 'I am fine! ', 'Foo. ', 'Bar. ', 'Baz. ', 'What? ', 'Nyan! ', 'Ni ji chi san. ', 'Nyan. ', 'Arigato']\n"
     ]
    }
   ],
   "source": [
    "print([x.text for x in nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am fine!  Foo.  Bar.  Baz.  What?  Nyan!  Ni ji chi san. \n"
     ]
    }
   ],
   "source": [
    "print(nodes[5].metadata[\"window\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "embed_model = OpenAIEmbedding(model='text-embedding-ada-002')\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key='window',\n",
    "    original_text_metadata_key='original_text'\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.node_parser = node_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index = VectorStoreIndex.from_documents([document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index.storage_context.persist(persist_dir='sentence_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code is optional to check\n",
    "# if an index file exist, then it will load it\n",
    "# if not, it will rebuild it\n",
    "\n",
    "import os\n",
    "from llama_index import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index import load_index_from_storage\n",
    "\n",
    "if not os.path.exists(\"sentence_index\"):\n",
    "    sentence_index = VectorStoreIndex.from_documents(\n",
    "        [document], service_context=sentence_context\n",
    "    )\n",
    "\n",
    "    sentence_index.storage_context.persist(persist_dir=\"sentence_index\")\n",
    "else:\n",
    "    sentence_index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=\"sentence_index\"),\n",
    "        service_context=sentence_context\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Preprocessor\n",
    "#### MetaDataReplacementPreprocessor \n",
    "##### Takes data stored in the metadata window and replace that data with the node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "post_proc = MetadataReplacementPostProcessor(target_metadata_key='window')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import NodeWithScore\n",
    "from copy import deepcopy\n",
    "\n",
    "scored_nodes = [NodeWithScore(node=x, score=1.0) for x in nodes]\n",
    "nodes_old = [deepcopy(n) for n in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How are you? '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_old[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_nodes = post_proc.postprocess_nodes(scored_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello.  How are you?  I am fine!  Foo.  Bar. \n"
     ]
    }
   ],
   "source": [
    "print(replaced_nodes[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranker\n",
    "{It takes the query and the retreived chunks and reorders them on the basis of the relevance}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.postprocessor import SentenceTransformerRerank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank = SentenceTransformerRerank(top_n=2, model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.schema import TextNode, NodeWithScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = QueryBundle('I want a dog')\n",
    "\n",
    "scored_nodes = [\n",
    "    NodeWithScore(node=TextNode(text='I want a cat'), score=0.8),\n",
    "    NodeWithScore(node=TextNode(text='I want a dog'), score=0.4)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_nodes = rerank.postprocess_nodes(\n",
    "    scored_nodes, query_bundle=query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I want a dog', 7.7261), ('I want a cat', -3.164504)]\n"
     ]
    }
   ],
   "source": [
    "print([(x.text, x.score) for x in reranked_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_window_engine = sentence_index.as_query_engine(\n",
    "    similarity_top_k=6, node_postprocessors=[post_proc, rerank]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** In the first stage of the pipeline in their methodology, the small language model is utilized to rank the elements present on a webpage. This ranking task involves selecting the top-k candidate elements based on the task description, the snapshot of the webpage at a specific step, and the actions performed in the preceding steps. The small language model acts as a classifier, assigning scores to the candidate elements to determine the most promising ones for further processing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** In the first stage of the pipeline, a fine-tuned small language model (LM) is utilized to rank the elements present on a webpage. This process involves treating candidate generation as a ranking task, where the goal is to select the top-k HTML elements based on the task description, the snapshot of the webpage at a specific step, and the actions performed in the preceding steps. The small LM is responsible for scoring and ranking these candidate elements, ultimately producing a small pool of promising candidates for further processing in the subsequent stage of the methodology."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Describe the processing of the small language part in detail that is used in the first stage of pipeline in their methodology\"\n",
    "\n",
    "sentence_response = sentence_window_engine.query(query)\n",
    "basic_response = query_engine.query(query)\n",
    "\n",
    "display_response(basic_response)\n",
    "display_response(sentence_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting all the code together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import ServiceContext, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.core.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.core.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "embed_model = OpenAIEmbedding(model='text-embedding-ada-002')\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key='window',\n",
    "    original_text_metadata_key='original_text'\n",
    ")\n",
    "\n",
    "\n",
    "def build_sentence_window_index(\n",
    "    documents,\n",
    "    llm,\n",
    "    embed_model=embed_model,\n",
    "    sentence_window_size=3,\n",
    "    save_dir=\"sentence_index\",\n",
    "):\n",
    "    # create the sentence window node parser w/ default settings\n",
    "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=sentence_window_size,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "    )\n",
    "    sentence_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        node_parser=node_parser,\n",
    "    )\n",
    "    if not os.path.exists(save_dir):\n",
    "        sentence_index = VectorStoreIndex.from_documents(\n",
    "            documents, service_context=sentence_context\n",
    "        )\n",
    "        sentence_index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        sentence_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=save_dir),\n",
    "            service_context=sentence_context,\n",
    "        )\n",
    "\n",
    "    return sentence_index\n",
    "\n",
    "\n",
    "def get_sentence_window_query_engine(\n",
    "    sentence_index, similarity_top_k=6, rerank_top_n=2\n",
    "):\n",
    "    # define postprocessors\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\"\n",
    "    )\n",
    "\n",
    "    sentence_window_engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]\n",
    "    )\n",
    "    return sentence_window_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Tru\n",
    "\n",
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"data/Mind2Web.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "document = Document(text='\\n\\n'.join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: 4ef8e74b-4d60-4a54-9e36-865caded8ea1\n",
      "Text: MIND2W EB: Towards a Generalist Agent for the Web Xiang Dengâˆ—Yu\n",
      "Gu Boyuan Zheng Shijie Chen Samuel Stevens Boshi Wang Huan Sunâˆ—Yu Suâˆ—\n",
      "The Ohio State University https://osu-nlp-group.github.io/Mind2Web\n",
      "Abstract We introduce MIND2W EB, the first dataset for developing and\n",
      "evaluating generalist agents for the web that can follow language\n",
      "instructio...\n"
     ]
    }
   ],
   "source": [
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4h/c8xgg6gj0_jbc09wh4qlg7sw0000gn/T/ipykernel_14905/3945329504.py:30: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  sentence_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "sentence_index = build_sentence_window_index(\n",
    "    document,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    save_dir='sentence_index'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_window_engine = get_sentence_window_query_engine(sentence_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** You can create an AI portfolio by developing and evaluating generalist agents for the web using datasets like MIND2W EB. Leveraging the power of large language models, such as in the case of MINDACT, can be effective in tackling tasks related to web understanding and action taking. Additionally, integrating multi-modal information, exploring reinforcement learning with feedback from real websites, and utilizing specialized language models for web-related tasks are promising directions to consider for building your AI portfolio."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = sentence_window_engine.query(\n",
    "    \"How do you create AI portfolio?\"\n",
    ")\n",
    "display_response(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation using Truelens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quesation Answer Relevence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval  import OpenAI as fOpenAI\n",
    "\n",
    "provider = fOpenAI(api_key=os.environ['OPENAI_API_KEY_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Answer Relevencee, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Answer Relevencee, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Feedback\n",
    "\n",
    "f_qa_relevence = Feedback(\n",
    "    provider.relevance_with_cot_reasons, name='Answer Relevencee'\n",
    "\n",
    ").on_input().on_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context-Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from trulens_eval import TruLlama\n",
    "\n",
    "context_selection = TruLlama.select_source_nodes().node.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Contextt Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Contextt Relevance, input context will be set to __record__.app.query.rets.source_nodes[:].node.text .\n"
     ]
    }
   ],
   "source": [
    "f_qs_relevance = (\n",
    "    Feedback(provider.qs_relevance_with_cot_reasons, name='Contextt Relevance').on_input().on(context_selection).aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groundedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ibnabeeali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.feedback import Groundedness\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Groundedness, input source will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "âœ… In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name='Groundedness')\n",
    "    .on(context_selection).on_output().aggregate(grounded.grounded_statements_aggregator)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruLlama\n",
    "from trulens_eval import FeedbackMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_recorder = TruLlama(\n",
    "    sentence_window_engine,\n",
    "    app_id='App_1',\n",
    "    feedbacks=[\n",
    "        f_qa_relevence, \n",
    "        f_qs_relevance,\n",
    "        f_groundedness\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = []\n",
    "with open('data/eval_questions.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Remove newline character and convert to integer\n",
    "        item = line.strip()\n",
    "        eval_questions.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the primary objective of MIND2WEB?',\n",
       " 'How many tasks were collected in MIND2WEB, and from how many websites?',\n",
       " 'What are the three necessary ingredients provided by MIND2WEB for building generalist web agents?',\n",
       " 'How does MIND2WEB differ from existing datasets for web agents?',\n",
       " 'What are the desiderata for a generalist agent for the web according to the paper?',\n",
       " 'What challenges are associated with building an agent for the web, as discussed in the paper?',\n",
       " 'What is MINDACT, and how does it address the challenges of using large language models (LLMs) for web agents?',\n",
       " 'What are the key features of the MIND2WEB dataset?',\n",
       " 'How does the data collection process for MIND2WEB differ from existing datasets?',\n",
       " 'What are the unique research challenges presented by MIND2WEB for developing generalist agents for the web?',\n",
       " 'What is the approach used for task demonstration in MIND2WEB?',\n",
       " 'How many tasks were verified and retained in MIND2WEB after the data collection process?',\n",
       " 'How does MIND2WEB compare to existing datasets in terms of domains covered, environments, and task information?',\n",
       " 'What are the objectives of MINDACT, and how does it leverage the data from MIND2WEB?',\n",
       " 'What are the two stages of the MINDACT framework, and what is the role of each stage?',\n",
       " 'How does MINDACT generate candidate elements for action prediction?',\n",
       " 'What is the role of small language models (LMs) in the MINDACT framework?',\n",
       " 'How does MINDACT utilize large language models (LLMs) for action prediction?',\n",
       " 'What are the main results reported for MINDACT in terms of element accuracy and operation F1 score?',\n",
       " 'How does MINDACT perform in cross-task and cross-website settings compared to the classification and generation baselines?']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in eval_questions:\n",
    "    with tru_recorder as recording:\n",
    "        sentence_window_engine.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[])\n",
    "records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>Answer Relevencee</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Contextt Relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"What is the primary objective of MIND2WEB?\"</td>\n",
       "      <td>\"The primary objective of MIND2WEB is for the agent to complete specific tasks on target websites through a series of actions, with each instance in the dataset containing task descriptions, action sequences, and webpage snapshots to facilitate the development of agents that can comprehend and carry out tasks in a more autonomous fashion.\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"How many tasks were collected in MIND2WEB, and from how many websites?\"</td>\n",
       "      <td>\"In MIND2WEB, a total of 2,411 tasks were collected from 137 websites.\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"What are the three necessary ingredients provided by MIND2WEB for building generalist web agents?\"</td>\n",
       "      <td>\"The three necessary ingredients provided by MIND2WEB for building generalist web agents are diverse domains, websites, and tasks; use of real-world websites instead of simulated and simplified ones; and a broad spectrum of user interaction patterns.\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"How does MIND2WEB differ from existing datasets for web agents?\"</td>\n",
       "      <td>\"MIND2W EB differs from existing datasets for web agents by providing a diverse range of domains, websites, and tasks, the use of real-world websites instead of simulated ones, and a broad spectrum of user interaction patterns.\"</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"What are the desiderata for a generalist agent for the web according to the paper?\"</td>\n",
       "      <td>\"The desiderata for a generalist agent for the web according to the paper are that it should be able to work on any website on the Internet and handle real-world websites that are dynamic, complex, and noisy.\"</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"What challenges are associated with building an agent for the web, as discussed in the paper?\"</td>\n",
       "      <td>\"Challenges associated with building an agent for the web include the need to process long and highly structured documents effectively, the difficulty of planning and grounding when only high-level goals are available, the potential bias in data collection towards English-language websites primarily used in the U.S., the limitation of modeling the web environment using only textual context without considering visual information, the need to effectively model interaction dynamics and dynamic environment transformations, the lack of flexibility in human-agent interaction where users may want to adjust task requirements, the limitations of evaluating systems with cached offline environments leading to potential false negatives, and the importance of addressing safety considerations for real-world deployment such as managing sensitive actions and preventing security breaches.\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"What is MINDACT, and how does it address the challenges of using large language models (LLMs) for web agents?\"</td>\n",
       "      <td>\"MINDACT is a two-stage model introduced to address the challenges of using large language models (LLMs) for web agents. The first stage of MINDACT involves using a fine-tuned small LM to filter the web elements, selecting a small pool of promising candidates. In the second stage, these candidate elements are consolidated to form a representative snippet of the webpage, which is then processed by an LLM to predict the final action, including predicting both the element for interaction and the corresponding operation. This approach allows for the efficient handling of the vast amount of elements present in HTML documents of real-world webpages, which would otherwise be infeasible or too costly to directly feed into an LLM's context.\"</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"What are the key features of the MIND2WEB dataset?\"</td>\n",
       "      <td>\"The key features of the MIND2WEB dataset include diverse domains, websites, and tasks, the use of real-world websites instead of simulated ones, and a broad spectrum of user interaction patterns.\"</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"How does the data collection process for MIND2WEB differ from existing datasets?\"</td>\n",
       "      <td>\"The data collection process for MIND2WEB differs from existing datasets in several ways. It covers a wide range of websites from various domains, totaling 137 websites from 31 domains, allowing for comprehensive testing of an agent's ability to generalize across diverse environments. Unlike previous studies, MIND2WEB utilizes real-world websites without manual simplification, resulting in environments with complexity that better reflects the intricacy of the modern web. Additionally, the dataset includes over 1,000 elements per page embedded within complex DOM structures, posing a significant challenge for modeling how to effectively process such long and highly structured documents. Lastly, MIND2WEB tasks are open-ended, exploring different functionalities of websites, unlike prior studies that primarily focus on testing the agent's ability to translate low-level instructions into actions.\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"What are the unique research challenges presented by MIND2WEB for developing generalist agents for the web?\"</td>\n",
       "      <td>\"The unique research challenges presented by MIND2WEB for developing generalist agents for the web include testing an agent's ability to generalize across varied environments spanning multiple websites and domains without manual simplification, dealing with the complexity of real-world websites with over 1,000 elements per page, and addressing the challenge of planning and grounding when only high-level goals are available instead of step-by-step directives.\"</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"What is the approach used for task demonstration in MIND2WEB?\"</td>\n",
       "      <td>\"The approach used for task demonstration in MIND2WEB involves developing a sophisticated annotation tool using Playwright and hiring annotators through Amazon Mechanical Turk. The annotators are presented with a target website, a brief description of the website, and sample tasks associated with it. They are then tasked with proposing open-ended and realistic tasks based on specific criteria, such as diverse types, multiple interaction rounds, and high-level goal descriptions instead of step-by-step instructions.\"</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"How many tasks were verified and retained in MIND2WEB after the data collection process?\"</td>\n",
       "      <td>\"After the data collection process in MIND2WEB, a total of 2,411 tasks were initially collected. Following verification, 61 tasks were discarded. This resulted in 2,350 tasks being retained after the verification process.\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"How does MIND2WEB compare to existing datasets in terms of domains covered, environments, and task information?\"</td>\n",
       "      <td>\"MIND2W EB distinguishes itself from existing datasets in several ways. Firstly, it spans across 31 domains with 137 websites, allowing comprehensive testing of an agent's ability in generalizing across varied environments. Secondly, it utilizes real-world websites without manual simplification, resulting in environments that exhibit complexity surpassing previous studies. Lastly, MIND2W EB directs annotators to propose open-ended tasks exploring different website functionalities, unlike studies that primarily focus on translating low-level instructions into actions.\"</td>\n",
       "      <td>0.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"What are the objectives of MINDACT, and how does it leverage the data from MIND2WEB?\"</td>\n",
       "      <td>\"The objectives of MINDACT are to develop an exploratory framework for web agents and leverage the data from MIND2W EB. MINDACT aims to utilize the power of large language models (LLMs) to effectively tackle tasks related to web environments. It employs a two-stage process where a small language model is first used to rank elements on a webpage, generating a pool of promising candidates. These candidate elements are then consolidated to form a representative snippet of the webpage, which is processed by a large language model to predict the final action, including identifying the element for interaction and the corresponding operation.\"</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"What are the two stages of the MINDACT framework, and what is the role of each stage?\"</td>\n",
       "      <td>\"The MINDACT framework consists of two stages. In the first stage, a fine-tuned small language model is used to rank the elements present on a webpage, generating a small pool of promising candidates. In the second stage, these candidate elements are consolidated to form a representative snippet of the webpage, which is then processed by a large language model to predict the final action, including both the element for interaction and the corresponding operation.\"</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"How does MINDACT generate candidate elements for action prediction?\"</td>\n",
       "      <td>\"MINDACT generates candidate elements for action prediction by utilizing a two-stage process. In the first stage, a fine-tuned small language model is used to rank the elements present on a webpage, resulting in a small pool of promising candidates. In the second stage, these candidate elements are consolidated to form a representative snippet of the webpage, which is then processed by a large language model to predict the final action, including both the element for interaction and the corresponding operation.\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"What is the role of small language models (LMs) in the MINDACT framework?\"</td>\n",
       "      <td>\"The role of small language models (LMs) in the MINDACT framework is to filter the web elements before passing them on to the large language models (LLMs) for further processing.\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\"How does MINDACT utilize large language models (LLMs) for action prediction?\"</td>\n",
       "      <td>\"MINDACT utilizes a two-stage model approach to leverage large language models (LLMs) for action prediction. Initially, a fine-tuned small LM is used to filter the web elements, followed by the utilization of an LLM to select from the filtered elements in a multi-choice question answering manner and predict the corresponding action associated with the selected element. This method has shown significant improvement over common modeling strategies used in prior work and demonstrates a decent level of generalization. Additionally, MINDACT is compatible with both open-source LLMs like Flan-T5 through fine-tuning and closed-source LLMs like GPT-3.5-turbo and GPT-4 through in-context learning.\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"What are the main results reported for MINDACT in terms of element accuracy and operation F1 score?\"</td>\n",
       "      <td>\"The main results reported for MINDACT in terms of element accuracy and operation F1 score are as follows:\\n- MINDACT with Flan-T5 B achieved an element accuracy of 43.6% and an operation F1 score of 76.8%.\\n- MINDACT with Flan-T5 L achieved an element accuracy of 53.4% and an operation F1 score of 75.7%.\\n- MINDACT with Flan-T5 XL achieved an element accuracy of 55.1% and an operation F1 score of 75.7%.\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\"How does MINDACT perform in cross-task and cross-website settings compared to the classification and generation baselines?\"</td>\n",
       "      <td>\"MINDACT performs better in the Cross-Task setting compared to the Cross-Website and Cross-Domain settings. In the Cross-Task setting, MINDACT achieves a step success rate of 52.0%, while in the Cross-Website and Cross-Domain settings, it achieves 38.9% and 39.6% respectively. This indicates that MINDACT has a higher success rate in tasks within the same website compared to tasks across different websites and domains.\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           input  \\\n",
       "0                                                                                   \"What is the primary objective of MIND2WEB?\"   \n",
       "1                                                       \"How many tasks were collected in MIND2WEB, and from how many websites?\"   \n",
       "2                            \"What are the three necessary ingredients provided by MIND2WEB for building generalist web agents?\"   \n",
       "3                                                              \"How does MIND2WEB differ from existing datasets for web agents?\"   \n",
       "4                                           \"What are the desiderata for a generalist agent for the web according to the paper?\"   \n",
       "5                                \"What challenges are associated with building an agent for the web, as discussed in the paper?\"   \n",
       "6                \"What is MINDACT, and how does it address the challenges of using large language models (LLMs) for web agents?\"   \n",
       "7                                                                           \"What are the key features of the MIND2WEB dataset?\"   \n",
       "8                                             \"How does the data collection process for MIND2WEB differ from existing datasets?\"   \n",
       "9                  \"What are the unique research challenges presented by MIND2WEB for developing generalist agents for the web?\"   \n",
       "10                                                               \"What is the approach used for task demonstration in MIND2WEB?\"   \n",
       "11                                    \"How many tasks were verified and retained in MIND2WEB after the data collection process?\"   \n",
       "12             \"How does MIND2WEB compare to existing datasets in terms of domains covered, environments, and task information?\"   \n",
       "13                                        \"What are the objectives of MINDACT, and how does it leverage the data from MIND2WEB?\"   \n",
       "14                                       \"What are the two stages of the MINDACT framework, and what is the role of each stage?\"   \n",
       "15                                                         \"How does MINDACT generate candidate elements for action prediction?\"   \n",
       "16                                                   \"What is the role of small language models (LMs) in the MINDACT framework?\"   \n",
       "17                                                \"How does MINDACT utilize large language models (LLMs) for action prediction?\"   \n",
       "18                         \"What are the main results reported for MINDACT in terms of element accuracy and operation F1 score?\"   \n",
       "19  \"How does MINDACT perform in cross-task and cross-website settings compared to the classification and generation baselines?\"   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       output  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \"The primary objective of MIND2WEB is for the agent to complete specific tasks on target websites through a series of actions, with each instance in the dataset containing task descriptions, action sequences, and webpage snapshots to facilitate the development of agents that can comprehend and carry out tasks in a more autonomous fashion.\"   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \"In MIND2WEB, a total of 2,411 tasks were collected from 137 websites.\"   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \"The three necessary ingredients provided by MIND2WEB for building generalist web agents are diverse domains, websites, and tasks; use of real-world websites instead of simulated and simplified ones; and a broad spectrum of user interaction patterns.\"   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \"MIND2W EB differs from existing datasets for web agents by providing a diverse range of domains, websites, and tasks, the use of real-world websites instead of simulated ones, and a broad spectrum of user interaction patterns.\"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \"The desiderata for a generalist agent for the web according to the paper are that it should be able to work on any website on the Internet and handle real-world websites that are dynamic, complex, and noisy.\"   \n",
       "5                       \"Challenges associated with building an agent for the web include the need to process long and highly structured documents effectively, the difficulty of planning and grounding when only high-level goals are available, the potential bias in data collection towards English-language websites primarily used in the U.S., the limitation of modeling the web environment using only textual context without considering visual information, the need to effectively model interaction dynamics and dynamic environment transformations, the lack of flexibility in human-agent interaction where users may want to adjust task requirements, the limitations of evaluating systems with cached offline environments leading to potential false negatives, and the importance of addressing safety considerations for real-world deployment such as managing sensitive actions and preventing security breaches.\"   \n",
       "6                                                                                                                                                                      \"MINDACT is a two-stage model introduced to address the challenges of using large language models (LLMs) for web agents. The first stage of MINDACT involves using a fine-tuned small LM to filter the web elements, selecting a small pool of promising candidates. In the second stage, these candidate elements are consolidated to form a representative snippet of the webpage, which is then processed by an LLM to predict the final action, including predicting both the element for interaction and the corresponding operation. This approach allows for the efficient handling of the vast amount of elements present in HTML documents of real-world webpages, which would otherwise be infeasible or too costly to directly feed into an LLM's context.\"   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \"The key features of the MIND2WEB dataset include diverse domains, websites, and tasks, the use of real-world websites instead of simulated ones, and a broad spectrum of user interaction patterns.\"   \n",
       "8   \"The data collection process for MIND2WEB differs from existing datasets in several ways. It covers a wide range of websites from various domains, totaling 137 websites from 31 domains, allowing for comprehensive testing of an agent's ability to generalize across diverse environments. Unlike previous studies, MIND2WEB utilizes real-world websites without manual simplification, resulting in environments with complexity that better reflects the intricacy of the modern web. Additionally, the dataset includes over 1,000 elements per page embedded within complex DOM structures, posing a significant challenge for modeling how to effectively process such long and highly structured documents. Lastly, MIND2WEB tasks are open-ended, exploring different functionalities of websites, unlike prior studies that primarily focus on testing the agent's ability to translate low-level instructions into actions.\"   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                             \"The unique research challenges presented by MIND2WEB for developing generalist agents for the web include testing an agent's ability to generalize across varied environments spanning multiple websites and domains without manual simplification, dealing with the complexity of real-world websites with over 1,000 elements per page, and addressing the challenge of planning and grounding when only high-level goals are available instead of step-by-step directives.\"   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                   \"The approach used for task demonstration in MIND2WEB involves developing a sophisticated annotation tool using Playwright and hiring annotators through Amazon Mechanical Turk. The annotators are presented with a target website, a brief description of the website, and sample tasks associated with it. They are then tasked with proposing open-ended and realistic tasks based on specific criteria, such as diverse types, multiple interaction rounds, and high-level goal descriptions instead of step-by-step instructions.\"   \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \"After the data collection process in MIND2WEB, a total of 2,411 tasks were initially collected. Following verification, 61 tasks were discarded. This resulted in 2,350 tasks being retained after the verification process.\"   \n",
       "12                                                                                                                                                                                                                                                                                                                                             \"MIND2W EB distinguishes itself from existing datasets in several ways. Firstly, it spans across 31 domains with 137 websites, allowing comprehensive testing of an agent's ability in generalizing across varied environments. Secondly, it utilizes real-world websites without manual simplification, resulting in environments that exhibit complexity surpassing previous studies. Lastly, MIND2W EB directs annotators to propose open-ended tasks exploring different website functionalities, unlike studies that primarily focus on translating low-level instructions into actions.\"   \n",
       "13                                                                                                                                                                                                                                                                       \"The objectives of MINDACT are to develop an exploratory framework for web agents and leverage the data from MIND2W EB. MINDACT aims to utilize the power of large language models (LLMs) to effectively tackle tasks related to web environments. It employs a two-stage process where a small language model is first used to rank elements on a webpage, generating a pool of promising candidates. These candidate elements are then consolidated to form a representative snippet of the webpage, which is processed by a large language model to predict the final action, including identifying the element for interaction and the corresponding operation.\"   \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                       \"The MINDACT framework consists of two stages. In the first stage, a fine-tuned small language model is used to rank the elements present on a webpage, generating a small pool of promising candidates. In the second stage, these candidate elements are consolidated to form a representative snippet of the webpage, which is then processed by a large language model to predict the final action, including both the element for interaction and the corresponding operation.\"   \n",
       "15                                                                                                                                                                                                                                                                                                                                                                                                      \"MINDACT generates candidate elements for action prediction by utilizing a two-stage process. In the first stage, a fine-tuned small language model is used to rank the elements present on a webpage, resulting in a small pool of promising candidates. In the second stage, these candidate elements are consolidated to form a representative snippet of the webpage, which is then processed by a large language model to predict the final action, including both the element for interaction and the corresponding operation.\"   \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \"The role of small language models (LMs) in the MINDACT framework is to filter the web elements before passing them on to the large language models (LLMs) for further processing.\"   \n",
       "17                                                                                                                                                                                                                  \"MINDACT utilizes a two-stage model approach to leverage large language models (LLMs) for action prediction. Initially, a fine-tuned small LM is used to filter the web elements, followed by the utilization of an LLM to select from the filtered elements in a multi-choice question answering manner and predict the corresponding action associated with the selected element. This method has shown significant improvement over common modeling strategies used in prior work and demonstrates a decent level of generalization. Additionally, MINDACT is compatible with both open-source LLMs like Flan-T5 through fine-tuning and closed-source LLMs like GPT-3.5-turbo and GPT-4 through in-context learning.\"   \n",
       "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \"The main results reported for MINDACT in terms of element accuracy and operation F1 score are as follows:\\n- MINDACT with Flan-T5 B achieved an element accuracy of 43.6% and an operation F1 score of 76.8%.\\n- MINDACT with Flan-T5 L achieved an element accuracy of 53.4% and an operation F1 score of 75.7%.\\n- MINDACT with Flan-T5 XL achieved an element accuracy of 55.1% and an operation F1 score of 75.7%.\"   \n",
       "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \"MINDACT performs better in the Cross-Task setting compared to the Cross-Website and Cross-Domain settings. In the Cross-Task setting, MINDACT achieves a step success rate of 52.0%, while in the Cross-Website and Cross-Domain settings, it achieves 38.9% and 39.6% respectively. This indicates that MINDACT has a higher success rate in tasks within the same website compared to tasks across different websites and domains.\"   \n",
       "\n",
       "    Answer Relevencee  Groundedness  Contextt Relevance  \n",
       "0                 1.0           0.5                0.90  \n",
       "1                 1.0           0.8                0.85  \n",
       "2                 1.0           1.0                0.85  \n",
       "3                 0.9           1.0                0.90  \n",
       "4                 0.8           1.0                0.85  \n",
       "5                 1.0           0.8                0.80  \n",
       "6                 0.9           1.0                0.80  \n",
       "7                 0.8           1.0                0.85  \n",
       "8                 1.0           NaN                0.80  \n",
       "9                 0.9           0.0                0.85  \n",
       "10                0.8           NaN                0.80  \n",
       "11                1.0           NaN                 NaN  \n",
       "12                0.9           NaN                 NaN  \n",
       "13                0.8           NaN                 NaN  \n",
       "14                0.8           NaN                 NaN  \n",
       "15                1.0           NaN                 NaN  \n",
       "16                NaN           NaN                 NaN  \n",
       "17                NaN           NaN                 NaN  \n",
       "18                NaN           NaN                 NaN  \n",
       "19                NaN           NaN                 NaN  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "records[[\"input\", \"output\"] + feedback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer Relevencee</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Contextt Relevance</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>App_1</th>\n",
       "      <td>0.905</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.7925</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Answer Relevencee  Groundedness  Contextt Relevance  latency  \\\n",
       "app_id                                                                 \n",
       "App_1               0.905          0.81              0.7925      3.1   \n",
       "\n",
       "        total_cost  \n",
       "app_id              \n",
       "App_1          0.0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datumspeak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
