{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.environ['OPENAI_API_KEY_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=['data/Mind2Web.pdf']\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge into a single document for better retrieval\n",
    "from llama_index.core import Document\n",
    "\n",
    "document = Document(\n",
    "    text='\\n\\n'.join([doc.text for doc in documents])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-Merge Retriever needs Hierarchical Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import HierarchicalNodeParser\n",
    "\n",
    "# create the hierarchical node parser w/ default settings\n",
    "node_parser = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes=[2048, 512, 128]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contains all nodes parent intermediate and leaf nodes\n",
    "nodes = node_parser.get_nodes_from_documents([document])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need only leaf nodes for making index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each instance in our dataset contains three components:\n",
      "Task description, which outlines the high-level goal of the task. We intentionally avoid low-level,\n",
      "step-by-step instructions, aiming to foster the development of agents that can comprehend and carry\n",
      "out tasks in a more autonomous fashion, rather than merely following prescriptive directives.\n",
      "Action sequence , which is the sequence of actions required to accomplish the task on the website.\n",
      "Each action in the sequence comprises a (Target Element, Operation) pair. The Target\n",
      "Element is an interactable element on the current webpage, and the Operation refers to the action\n",
      "to be executed on that element.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import get_leaf_nodes\n",
    "\n",
    "leaf_nodes = get_leaf_nodes(nodes)\n",
    "print(leaf_nodes[30].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Parent Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To achieve this, our approach for data collection adheres\n",
      "to the following principles. Firstly, instead of recreating websites in simulation, which often leads to\n",
      "oversimplified environments, we engage directly with real-world websites and capture snapshots of\n",
      "these environments. Secondly, we collate a diverse set of websites from varied domains and crowd-\n",
      "source realistic tasks that cover a wide range of functionalities provided by these websites. Finally,\n",
      "acknowledging the challenge of perfectly replicating the complexity of real-world environments, we\n",
      "strive to capture a comprehensive snapshot of each website and the full interaction trace, to the extent\n",
      "that all the tasks can be seamlessly replayed offline. This supports rich modeling and evaluation\n",
      "approaches, ensuring a robust and practical dataset for research.\n",
      "2.1 Task Definition\n",
      "The primary objective of MIND2W EBis for the agent to complete a specific task on the target website\n",
      "through a series of actions. Each instance in our dataset contains three components:\n",
      "Task description, which outlines the high-level goal of the task. We intentionally avoid low-level,\n",
      "step-by-step instructions, aiming to foster the development of agents that can comprehend and carry\n",
      "out tasks in a more autonomous fashion, rather than merely following prescriptive directives.\n",
      "Action sequence , which is the sequence of actions required to accomplish the task on the website.\n",
      "Each action in the sequence comprises a (Target Element, Operation) pair. The Target\n",
      "Element is an interactable element on the current webpage, and the Operation refers to the action\n",
      "to be executed on that element. We support three common operations: Click (also including Hover\n",
      "andPress Enter ),Type , and Select Option . For Type andSelect Option , they also require\n",
      "an additional value as an argument. Actions in a sequence often span multiple webpages of a website.\n",
      "3\n",
      "\n",
      "Task Description:Show me the reviews for the auto repair business closest to 10002.ActionSequence:Target ElementOperation1.[searchbox] FindTYPE: auto repair2.[button] Auto RepairCLICK3.[textbox] NearTYPE:100024.[button] 10002CLICK5.[button] SearchCLICK6.[switch] Show BBB Accredited onlyCLICK7.[svg]CLICK8.[button] Sort ByCLICK9.[link] Fast Lane 24 Hour Auto RepairCLICK10.\n"
     ]
    }
   ],
   "source": [
    "nodes_by_id = {node.node_id: node for node in nodes}\n",
    "\n",
    "parent_node = nodes_by_id[leaf_nodes[30].parent_node.node_id]\n",
    "print(parent_node.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.core.settings import Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "embed_model = OpenAIEmbedding(model='text-embedding-ada-002')\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.node_parser = node_parser\n",
    "\n",
    "\n",
    "# auto_merging_context = ServiceContext.from_defaults(\n",
    "#     llm=llm,\n",
    "#     embed_model=embed_model,\n",
    "#     node_parser=node_parser,\n",
    "# )\n",
    "\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "automerging_index = VectorStoreIndex(\n",
    "    leaf_nodes, storage_context=storage_context\n",
    ")\n",
    "\n",
    "automerging_index.storage_context.persist(persist_dir=\"./merging_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code is optional to check\n",
    "# if an index file exist, then it will load it\n",
    "# if not, it will rebuild it\n",
    "\n",
    "import os\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "if not os.path.exists(\"./merging_index\"):\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "    automerging_index = VectorStoreIndex(\n",
    "            leaf_nodes,\n",
    "            storage_context=storage_context,\n",
    "            service_context=auto_merging_context\n",
    "        )\n",
    "\n",
    "    automerging_index.storage_context.persist(persist_dir=\"./merging_index\")\n",
    "else:\n",
    "    automerging_index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=\"./merging_index\"),\n",
    "        service_context=auto_merging_context\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "automerging_retriever = automerging_index.as_retriever(\n",
    "    similarity_top_k=12\n",
    ")\n",
    "\n",
    "retriever = AutoMergingRetriever(\n",
    "    automerging_retriever, \n",
    "    automerging_index.storage_context, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "rerank = SentenceTransformerRerank(top_n=6, model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\")\n",
    "\n",
    "auto_merging_engine = RetrieverQueryEngine.from_args(\n",
    "    automerging_retriever, node_postprocessors=[rerank]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_merging_response = auto_merging_engine.query(\n",
    "    'What is the accuracy of the mini2web methodology on unseen websites? And if 38.9% / 39.6% then why is it out of 39.6 and not out of 100'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The accuracy of the mini2web methodology on unseen websites is 38.9% / 39.6%. It is expressed as a percentage out of 39.6 and not out of 100 because the evaluation metric used in this context is the step success rate, which is specifically calculated based on the number of successful steps taken by the model in completing a task."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(auto_merging_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from llama_index.core import (\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.node_parser import HierarchicalNodeParser\n",
    "from llama_index.core.node_parser import get_leaf_nodes\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "from llama_index.core.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "def build_automerging_index(\n",
    "    documents,\n",
    "    llm,\n",
    "    embed_model,\n",
    "    save_dir=\"merging_index\",\n",
    "    chunk_sizes=None,\n",
    "):\n",
    "    chunk_sizes = chunk_sizes or [2048, 512, 128]\n",
    "    node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=chunk_sizes)\n",
    "    nodes = node_parser.get_nodes_from_documents(documents)\n",
    "    leaf_nodes = get_leaf_nodes(nodes)\n",
    "\n",
    "    Settings.llm = llm\n",
    "    Settings.embed_model = embed_model\n",
    "    Settings.node_parser = node_parser\n",
    "\n",
    "    # merging_context = ServiceContext.from_defaults(\n",
    "    #     llm=llm,\n",
    "    #     embed_model=embed_model,\n",
    "    # )\n",
    "\n",
    "\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        automerging_index = VectorStoreIndex(\n",
    "            leaf_nodes, storage_context=storage_context\n",
    "            )\n",
    "        automerging_index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        automerging_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=save_dir),\n",
    "        )\n",
    "    return automerging_index\n",
    "\n",
    "\n",
    "def get_automerging_query_engine(\n",
    "    automerging_index,\n",
    "    similarity_top_k=12,\n",
    "    rerank_top_n=6,\n",
    "):\n",
    "    base_retriever = automerging_index.as_retriever(similarity_top_k=similarity_top_k)\n",
    "    retriever = AutoMergingRetriever(\n",
    "        base_retriever, automerging_index.storage_context, verbose=True\n",
    "    )\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\"\n",
    "    )\n",
    "    auto_merging_engine = RetrieverQueryEngine.from_args(\n",
    "        retriever, node_postprocessors=[rerank]\n",
    "    )\n",
    "    return auto_merging_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "index = build_automerging_index(\n",
    "    [document],\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n",
    "    embed_model=OpenAIEmbedding(model=\"text-embedding-ada-002\"),\n",
    "    save_dir=\"merging_index\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = get_automerging_query_engine(index, similarity_top_k=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trulens Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Tru\n",
    "\n",
    "Tru().reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_merging_index_0 = build_automerging_index(\n",
    "    documents,\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n",
    "    embed_model=OpenAIEmbedding(model=\"text-embedding-ada-002\"),\n",
    "    save_dir=\"merging_index_0\",\n",
    "    chunk_sizes=[2048,512],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_merging_engine_0 = get_automerging_query_engine(\n",
    "    auto_merging_index_0,\n",
    "    similarity_top_k=12,\n",
    "    rerank_top_n=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval  import OpenAI as fOpenAI\n",
    "\n",
    "provider = fOpenAI(api_key=os.environ['OPENAI_API_KEY_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Contextt Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Contextt Relevance, input context will be set to __record__.app.query.rets.source_nodes[:].node.text .\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from trulens_eval import TruLlama\n",
    "from trulens_eval import Feedback\n",
    "\n",
    "context_selection = TruLlama.select_source_nodes().node.text\n",
    "\n",
    "f_qs_relevance = (\n",
    "    Feedback(provider.qs_relevance_with_cot_reasons, name='Contextt Relevance').on_input().on(context_selection).aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Answer Relevencee, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Answer Relevencee, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Feedback\n",
    "\n",
    "f_qa_relevence = Feedback(\n",
    "    provider.relevance_with_cot_reasons, name='Answer Relevencee'\n",
    "\n",
    ").on_input().on_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Groundedness, input source will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "âœ… In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ibnabeeali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.feedback import Groundedness\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=provider)\n",
    "\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name='Groundedness')\n",
    "    .on(context_selection).on_output().aggregate(grounded.grounded_statements_aggregator)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruLlama\n",
    "from trulens_eval import Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'relevance_with_cot_reasons'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m qa_relevance \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 2\u001b[0m     Feedback(\u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelevance_with_cot_reasons\u001b[49m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer Relevance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mon_input_output()\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m qs_relevance \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      7\u001b[0m     Feedback(openai\u001b[38;5;241m.\u001b[39mrelevance_with_cot_reasons, name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext Relevance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mon_input()\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mon(TruLlama\u001b[38;5;241m.\u001b[39mselect_source_nodes()\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39maggregate(np\u001b[38;5;241m.\u001b[39mmean)\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#grounded = Groundedness(groundedness_provider=openai, summarize_provider=openai)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'relevance_with_cot_reasons'"
     ]
    }
   ],
   "source": [
    "\n",
    "qa_relevance = (\n",
    "    Feedback(openai.relevance_with_cot_reasons, name=\"Answer Relevance\")\n",
    "    .on_input_output()\n",
    ")\n",
    "\n",
    "qs_relevance = (\n",
    "    Feedback(openai.relevance_with_cot_reasons, name = \"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(TruLlama.select_source_nodes().node.text)\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "#grounded = Groundedness(groundedness_provider=openai, summarize_provider=openai)\n",
    "grounded = Groundedness(groundedness_provider=openai)\n",
    "\n",
    "groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n",
    "        .on(TruLlama.select_source_nodes().node.text)\n",
    "        .on_output()\n",
    "        .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "feedbacks = [qa_relevance, qs_relevance, groundedness]\n",
    "\n",
    "def get_prebuilt_trulens_recorder(query_engine, app_id):\n",
    "    tru_recorder = TruLlama(\n",
    "        query_engine,\n",
    "        app_id=app_id,\n",
    "        feedbacks=feedbacks\n",
    "        )\n",
    "    return tru_recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_prebuilt_trulens_recorder\n\u001b[1;32m      3\u001b[0m tru_recorder \u001b[38;5;241m=\u001b[39m get_prebuilt_trulens_recorder(\n\u001b[1;32m      4\u001b[0m     auto_merging_engine_0,\n\u001b[1;32m      5\u001b[0m     app_id \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapp_0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/VSCode/DatumSpeak/utils.py:31\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUGGINGFACE_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m _ \u001b[38;5;241m=\u001b[39m load_dotenv(find_dotenv())\n\u001b[0;32m---> 31\u001b[0m openai \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOPENAI_API_KEY_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m qa_relevance \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     36\u001b[0m     Feedback(openai\u001b[38;5;241m.\u001b[39mrelevance_with_cot_reasons, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer Relevance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;241m.\u001b[39mon_input_output()\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     40\u001b[0m qs_relevance \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     41\u001b[0m     Feedback(openai\u001b[38;5;241m.\u001b[39mrelevance_with_cot_reasons, name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext Relevance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;241m.\u001b[39mon_input()\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;241m.\u001b[39mon(TruLlama\u001b[38;5;241m.\u001b[39mselect_source_nodes()\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;241m.\u001b[39maggregate(np\u001b[38;5;241m.\u001b[39mmean)\n\u001b[1;32m     45\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/datumspeak/lib/python3.12/site-packages/trulens_eval/feedback/provider/openai.py:76\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, endpoint, pace, rpm, model_engine, *args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m self_kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     74\u001b[0m self_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_engine\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m model_engine\n\u001b[0;32m---> 76\u001b[0m self_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendpoint\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIEndpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrpm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrpm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mself_kwargs\n\u001b[1;32m     82\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/datumspeak/lib/python3.12/site-packages/trulens_eval/feedback/provider/endpoint/openai.py:251\u001b[0m, in \u001b[0;36mOpenAIEndpoint.__init__\u001b[0;34m(self, name, client, rpm, pace, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[CLASS_INFO]\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Pass kwargs to client.\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43moai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     self_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m OpenAIClient(client\u001b[38;5;241m=\u001b[39mclient)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/datumspeak/lib/python3.12/site-packages/openai/_client.py:104\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    102\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m     )\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from utils import get_prebuilt_trulens_recorder\n",
    "\n",
    "tru_recorder = get_prebuilt_trulens_recorder(\n",
    "    auto_merging_engine_0,\n",
    "    app_id ='app_0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = []\n",
    "with open('generated_questions.text', 'r') as file:\n",
    "    for line in file:\n",
    "        # Remove newline character and convert to integer\n",
    "        item = line.strip()\n",
    "        eval_questions.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evals(eval_questions, tru_recorder, query_engine):\n",
    "    for question in eval_questions:\n",
    "        with tru_recorder as recording:\n",
    "            response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evals(eval_questions, tru_recorder, auto_merging_engine_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "\n",
    "Tru().get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tru().run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datumspeak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
