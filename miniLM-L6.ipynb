{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY_']\n",
    "HUGGINGFACE_TOKEN = os.environ['HUGGINGFACE_TOKEN']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embedding_function = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'db/chroma'\n",
    "\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loading and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('data/ReAct.pdf')\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = splitter.split_documents(pages)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Published as a conference paper at ICLR 2023\\nREAC T: S YNERGIZING REASONING AND ACTING IN\\nLANGUAGE MODELS\\nShunyu Yao∗*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2\\n1Department of Computer Science, Princeton University\\n2Google Research, Brain team\\n1{shunyuy,karthikn}@princeton.edu\\n2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\\nABSTRACT\\nWhile large language models (LLMs) have demonstrated impressive performance\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action\\nplan generation) have primarily been studied as separate topics. In this paper, we\\nexplore the use of LLMs to generate both reasoning traces and task-speciﬁc actions\\nin an interleaved manner, allowing for greater synergy between the two: reasoning\\ntraces help the model induce, track, and update action plans as well as handle\\nexceptions, while actions allow it to interface with and gather additional information\\nfrom external sources such as knowledge bases or environments. We apply our\\napproach, named ReAct , to a diverse set of language and decision making tasks\\nand demonstrate its effectiveness over state-of-the-art baselines in addition to\\nimproved human interpretability and trustworthiness. Concretely, on question\\nanswering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent\\nissues of hallucination and error propagation in chain-of-thought reasoning by\\ninteracting with a simple Wikipedia API, and generating human-like task-solving\\ntrajectories that are more interpretable than baselines without reasoning traces.\\nFurthermore, on two interactive decision making benchmarks (ALFWorld and\\nWebShop), ReAct outperforms imitation and reinforcement learning methods by\\nan absolute success rate of 34% and 10% respectively, while being prompted with\\nonly one or two in-context examples.\\n1 I NTRODUCTION'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.42610213e-02 -1.00450464e-01  5.63753508e-02 -4.76643853e-02\n",
      "  3.94125842e-02  6.60193060e-03  6.64848685e-02  5.35975285e-02\n",
      "  9.59211141e-02  3.18779238e-02 -3.51582319e-02 -3.34922858e-02\n",
      "  5.00640534e-02  2.25127488e-02  1.10441603e-01 -1.65781528e-02\n",
      "  1.40233627e-02 -3.54926032e-03 -1.04830660e-01 -7.76339397e-02\n",
      "  6.04142994e-02 -2.94648502e-02  3.89858894e-02 -3.81193310e-02\n",
      " -1.00444049e-01 -2.74950191e-02  2.60499828e-02 -7.76545629e-02\n",
      "  5.04011922e-02  1.95956845e-02  6.54101297e-02  4.33976427e-02\n",
      "  1.21673066e-02  1.39522394e-02 -8.21192637e-02  1.36251256e-01\n",
      " -5.80241233e-02  2.52280006e-04 -1.92695986e-02 -9.64747369e-02\n",
      " -8.16828236e-02 -3.09433434e-02  5.04717380e-02 -5.84152602e-02\n",
      "  9.27214026e-02 -5.27057275e-02 -3.17007080e-02 -8.79268919e-04\n",
      " -7.07282647e-02  3.39548034e-03 -1.18072145e-01 -1.58097241e-02\n",
      "  1.02372235e-02 -1.07866814e-02  8.86400230e-03  5.31580951e-03\n",
      "  2.22819168e-02  4.35798541e-02  2.89247930e-02 -8.27872008e-02\n",
      " -9.46392938e-02 -9.24950466e-02 -7.56351138e-03 -4.70055789e-02\n",
      " -8.09441507e-02  1.46964323e-02  3.10390517e-02  2.76881773e-02\n",
      " -1.01707168e-02  6.34384900e-02 -1.59367861e-03  7.42008490e-03\n",
      " -6.46476150e-02  9.01245233e-03 -6.38296753e-02  9.55466367e-03\n",
      "  2.56486852e-02 -2.78413463e-02  2.51669530e-02 -4.48640659e-02\n",
      " -7.65155442e-03 -7.32815871e-03  1.69219393e-02  3.44312750e-02\n",
      "  7.95777217e-02  4.03937362e-02 -4.35277559e-02  1.08057791e-02\n",
      "  1.04721524e-01  2.48414911e-02 -1.10978754e-02 -8.33433121e-02\n",
      "  1.12796354e-03  5.19006327e-02  6.10213205e-02  6.59502149e-02\n",
      " -2.35633291e-02 -8.23631231e-03  9.27864108e-03  4.64499034e-02\n",
      "  9.32936743e-02  1.26054972e-01 -1.25463586e-03 -1.10520445e-01\n",
      " -1.63305663e-02 -1.99525990e-02  2.77211387e-02  1.70149188e-02\n",
      " -3.36646358e-03 -2.30882075e-02 -1.26538770e-02  1.73048838e-03\n",
      "  6.40754029e-02  5.65091297e-02  4.20659147e-02 -1.27332611e-02\n",
      "  4.91154306e-02  1.05487695e-02  6.10932359e-04 -8.15252494e-03\n",
      "  8.12117681e-02  5.43380044e-02  6.58737421e-02  2.56259050e-02\n",
      "  3.00403647e-02 -8.90068263e-02 -1.42964143e-02  4.15919803e-33\n",
      "  4.85389940e-02  8.40758253e-03  2.01848634e-02  3.34801339e-02\n",
      "  5.92131950e-02  2.29832320e-03  3.15445811e-02  6.10456243e-03\n",
      "  1.23287123e-02 -3.11361400e-05 -3.50208543e-02  1.36641879e-03\n",
      " -2.39247754e-02  7.12426156e-02  7.24016801e-02 -1.16004022e-02\n",
      " -4.71151806e-02  4.52164910e-04 -6.92157224e-02 -5.89332357e-02\n",
      "  6.01084121e-02 -1.02638789e-02  5.77762723e-02 -7.79828988e-03\n",
      "  3.96612473e-02  3.15652299e-03  8.45624432e-02 -4.87582013e-02\n",
      " -2.64856555e-02 -8.27308558e-03 -1.00104757e-01 -5.49252555e-02\n",
      " -4.61241826e-02  8.34467784e-02  1.67129040e-02 -4.90280911e-02\n",
      " -2.98849437e-02 -5.15242480e-02  4.81776334e-02 -2.06416193e-02\n",
      " -2.13898327e-02  5.19578382e-02  1.08167771e-02  7.58510232e-02\n",
      " -6.74122348e-02 -2.61104181e-02 -1.61644127e-02  4.77962904e-02\n",
      "  8.87658540e-03 -1.68381914e-04  4.12494838e-02  4.99089323e-02\n",
      "  6.67086244e-02 -5.82911521e-02  1.58109181e-02 -1.09687820e-02\n",
      "  2.03850260e-03  3.66323791e-03  7.16091506e-03  5.22570461e-02\n",
      "  1.59416236e-02  1.38704963e-02  3.96922976e-03  1.13813408e-01\n",
      "  4.43795770e-02  5.81991896e-02 -8.76433402e-02  1.01231635e-01\n",
      "  8.60403478e-02 -9.28415135e-02 -1.18260961e-02  1.47831133e-02\n",
      "  2.18298882e-02 -1.51503133e-02  3.53804603e-02 -7.35128522e-02\n",
      " -1.84460059e-02 -1.21162720e-01  8.10464192e-03  6.40229648e-03\n",
      "  7.67761888e-03 -1.48509955e-02 -6.84547052e-02  5.64049138e-03\n",
      " -2.16813665e-02 -5.62862679e-02  1.12690236e-02 -4.05948982e-02\n",
      " -1.05532268e-02 -2.76309736e-02 -3.11883017e-02 -1.89329684e-02\n",
      "  9.16041434e-03  4.02823696e-03 -2.65275873e-02 -5.25190268e-33\n",
      "  6.50529191e-03 -4.86451313e-02 -4.20631915e-02  4.21209335e-02\n",
      " -4.12734337e-02 -5.06718680e-02  6.10008724e-02 -2.57867817e-02\n",
      " -1.06541738e-02 -1.11992382e-01 -8.76573026e-02 -1.42974434e-02\n",
      "  1.96385458e-02  1.48932319e-02  4.64511514e-02 -9.21649262e-02\n",
      "  1.97621193e-02  2.49790922e-02 -3.62525089e-03  6.33716509e-02\n",
      "  2.09608190e-02  4.40863930e-02 -1.20272987e-01 -2.02444829e-02\n",
      " -8.52654651e-02  8.51930156e-02 -1.53537979e-02  4.28036638e-02\n",
      " -2.75964406e-03  8.46552327e-02 -3.33564095e-02 -5.52431159e-02\n",
      " -4.54832129e-02 -2.45346464e-02 -2.83942875e-02  1.15591735e-02\n",
      "  3.69034074e-02  2.45078430e-02 -6.30413443e-02  2.10801344e-02\n",
      "  4.93318737e-02 -8.32727179e-02 -1.14334775e-02  4.19764444e-02\n",
      " -1.02409991e-02  2.70640459e-02 -1.10013261e-01  1.01338094e-02\n",
      " -6.85190111e-02 -5.89666367e-02 -2.74516735e-02 -3.31310146e-02\n",
      " -4.29714359e-02 -5.33985272e-02 -4.20488417e-03 -2.49877013e-02\n",
      "  2.34326664e-02 -8.77160430e-02 -1.44730099e-02 -4.55106162e-02\n",
      " -4.25127670e-02  3.15025635e-02  3.66817899e-02 -7.53224418e-02\n",
      "  2.74460260e-02  5.36712296e-02 -2.31654551e-02 -6.39591962e-02\n",
      "  5.44785559e-02 -3.25235054e-02  4.61061969e-02 -2.54221968e-02\n",
      " -3.33290137e-02 -2.04558782e-02  9.20316055e-02 -1.53608602e-02\n",
      " -7.71122575e-02 -7.19150454e-02  5.63323358e-03 -4.60387059e-02\n",
      " -1.00018568e-02 -4.43955697e-03  1.38591351e-02  3.00554018e-02\n",
      " -4.35638838e-02  1.31026749e-02 -4.62514907e-02  1.06405079e-01\n",
      " -1.87562704e-02 -2.37663165e-02  1.48395104e-02  2.53075156e-02\n",
      "  4.45030555e-02  9.54092070e-02 -8.66861045e-02 -5.56510393e-08\n",
      " -4.05622199e-02  4.27582264e-02  3.42024229e-02  5.82610257e-02\n",
      "  4.47202325e-02 -8.29020888e-02 -7.04069659e-02 -4.90850350e-03\n",
      "  8.05109460e-03  1.76584180e-02  6.99400380e-02  5.60963200e-03\n",
      " -3.55831124e-02  6.67591300e-03  3.68093476e-02  6.50332915e-03\n",
      "  7.19263703e-02  1.65812466e-02 -4.96743396e-02 -7.70249590e-02\n",
      "  7.59032220e-02 -4.70554689e-03 -1.05842352e-01  7.24389330e-02\n",
      "  3.82977389e-02 -1.52423065e-02 -5.34103140e-02  1.52279526e-01\n",
      "  6.54100673e-04  1.05388779e-02 -1.39617943e-03 -3.53551209e-02\n",
      "  4.76524420e-03  6.41779378e-02  7.62402639e-02 -2.32835282e-02\n",
      "  3.84096950e-02 -6.75065666e-02  4.50177416e-02 -3.90196033e-02\n",
      "  2.16015652e-02  4.31639813e-02 -2.36253049e-02  5.52922785e-02\n",
      "  6.15100861e-02  2.89051863e-03 -1.02885701e-01 -1.10120699e-01\n",
      "  2.53191665e-02 -1.10397656e-02 -3.63177583e-02 -1.70582347e-02\n",
      " -2.99996994e-02  9.56835970e-02  3.20294276e-02  7.58199096e-02\n",
      " -6.76921103e-03 -2.74798721e-02 -1.91827603e-02  8.05341229e-02\n",
      "  9.93680134e-02  6.09623678e-02 -7.09990114e-02 -1.43162627e-02]\n"
     ]
    }
   ],
   "source": [
    "temp = embedding_function.encode(splits[0].page_content)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings and store them in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "documents = []\n",
    "metadata = []\n",
    "ids = []\n",
    "count = 0\n",
    "# .encode(\"hello how are you\")\n",
    "for split in splits:\n",
    "    embeddings.append(list((embedding_function.encode(split.page_content).astype(float))))\n",
    "    documents.append(split.page_content)\n",
    "    metadata.append(split.metadata)\n",
    "    ids.append(f\"id{count}\")\n",
    "    count += 1\n",
    "\n",
    "    \n",
    "\n",
    "# print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating VectorDB from Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.create_collection(name='ReActPaper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    embeddings=embeddings,\n",
    "    documents=documents,\n",
    "    metadatas=metadata,\n",
    "    ids=ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What does it do about hotpotQA and Wikipedia API\"\n",
    "results = collection.query(\n",
    "    query_texts=query,\n",
    "    n_results=5\n",
    ")\n",
    "len(results['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer to the above query: What does it do about hotpotQA and Wikipedia API is \n",
      " demonstrating a synergy of reasoning and acting.\n",
      "3.1 S ETUP\n",
      "Domains We consider two datasets challenging knowledge retrieval and reasoning: (1) Hot-\n",
      "PotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning\n",
      "over two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact veriﬁcation\n",
      "benchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based\n",
      "on if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only\n",
      "setup for both tasks, where models only receive the question/claim as input without access to support\n",
      "paragraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with\n",
      "an external environment to support reasoning.\n",
      "Action Space We design a simple Wikipedia web API with three types of actions to support\n",
      "interactive information retrieval: (1) search [entity ], which returns the ﬁrst 5 sentences from\n",
      "the corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the\n",
      "Wikipedia search engine, (2) lookup [string ], which would return the next sentence in the page\n",
      "containing string , simulating Ctrl+F functionality on the browser. (3) finish [answer ], which\n",
      "would ﬁnish the current task with answer . We note that this action space mostly can only retrieve a\n",
      "small part of a passage based on exact passage name, which is signiﬁcantly weaker than state-of-the-\n",
      "art lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,\n",
      "and force models to retrieve via explicit reasoning in language.\n",
      "3.2 M ETHODS\n",
      "ReAct Prompting For HotpotQA and Fever, we randomly select 6 and 3 cases2from the training\n",
      "set and manually compose ReAct -format trajectories to use as few-shot exemplars in the prompts.\n",
      "Similar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Answer to the above query: {query} is \\n {str(results['documents'][0][0])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Published as a conference paper at ICLR 2023\\nMethod Pick Clean Heat Cool Look Pick 2 All\\nAct (best of 6) 88 42 74 67 72 41 45\\nReAct (avg) 65 39 83 76 55 24 57\\nReAct (best of 6) 92 58 96 86 78 41 71\\nReAct-IM (avg) 55 59 60 55 23 24 48\\nReAct-IM (best of 6) 62 68 87 57 39 33 53\\nBUTLER g(best of 8) 33 26 70 76 17 12 22\\nBUTLER (best of 8) 46 39 74 100 22 24 37\\nTable 3: AlfWorld task-speciﬁc success rates (%). BUTLER and\\nBUTLER gresults are from Table 4 of Shridhar et al. (2020b). All\\nmethods use greedy decoding, except that BUTLER uses beam search.Method Score SR\\nAct 62.3 30.1\\nReAct 66.6 40.0\\nIL 59.9 29.1\\nIL+RL 62.4 28.7\\nHuman82.1 59.6Expert\\nTable 4: Score and suc-\\ncess rate (SR) on Web-\\nshop. IL/IL+RL taken\\nfrom Yao et al. (2022).\\ntrained with 1,012 human annotated trajectories, and a imitation + reinforcement learning (IL + RL)\\nmethod additionally trained with 10,587 training instructions.\\nResults ReAct outperforms Act on both ALFWorld (Table 3) and Webshop (Table 4). On\\nALFWorld, the best ReAct trial achieves an average success rate of 71%, signiﬁcantly outperforming\\nthe best Act (45%) and BUTLER (37%) trials. In fact, even the worse ReAct trial (48%) beats\\nthe best trial of both methods. Moreover, the advantage of ReAct overAct is consistent across\\nsix controlled trials, with relative performance gain ranging from 33% to 90% and averaging 62%.\\nQualitatively, we saw that, without any thoughts at all, Act fails to correctly decompose goals\\ninto smaller subgoals, or loses track of the current state of the environment. Example trajectories\\ncomparing ReAct andAct can be found in Appendix D.2.1 and Appendix D.2.2.\\nOn Webshop, one-shot Act prompting already performs on par with IL and IL+RL methods. With\\nadditional sparse reasoning, ReAct achieves signiﬁcantly better performance, with an absolute 10%\\nimprovement over the previous best success rate. By checking examples, we ﬁnd that ReAct is more', 'Action 3: Finish[San Marco]\\nObservation 3: Episode finished\\nCoT\\nQuestion: Was Vanderbilt University or Emory University founded first?\\nThought: Let’s think step by step. Vanderbilt University was founded in 1873. Emory\\nUniversity was founded in 1836. 1873 (Vanderbilt University) > 1836 (Emory\\nUniversity), so Vanderbilt University was founded first.\\nAnswer: Emory University\\nFailure: Reasoning error\\nReAct\\nQuestion: What was the middle name of the actress who was cast alongside Harvey\\nFierstein, Linda Hart, Dick Latessa, Clarke Thorell, Mary Bond Davis, Laura\\nBell Bundy, Matthew Morrison, Corey Reynolds, and Marissa Jaret Winokur in\\nHairspray?\\nThought 1: I need to search Harvey Fierstein, Linda Hart, Dick Latessa, Clarke Thorell,\\nMary Bond Davis, Laura Bell Bundy, Matthew Morrison, Corey Reynolds, and\\nMarissa Jaret Winokur, find the actress that was cast alongside them in\\nHairspray, then find its middle name.\\nAction 1: Search[Harvey Fierstein]\\nObservation 1: Harvey Forbes Fierstein ...\\nThought 2: Harvey Fierstein is an actor, playwright, and screenwriter. I need to\\nsearch Linda Hart next...\\nAction 2: Search[Linda Hart]...\\nCoT\\nQuestion: Who is older Danny Green or James Worthy?\\nThought: Let’s think step by step. Danny Green was born in 1987. James Worthy was\\nborn in 1961. 1987 (Danny Green) > 1961 (James Worthy), so Danny Green is\\nolder.\\nAnswer: Danny Green\\n32', \"Published as a conference paper at ICLR 2023\\n$FW\\x03\\x14\\x1d\\x037KLQN>)LUVW\\x03,\\x03QHHG\\x03WR\\x03ILQG\\x03D\\x03SHSSHU\\x03VKDNHU\\x11\\x11\\x11PRUH\\x03OLNHO\\\\\\x03WR\\x03DSSHDU\\x03LQ\\x03FDELQHWV\\x03\\x0b\\x14\\x10\\x19\\x0c\\x0f\\x03FRXQWHUWRSV\\x03\\x0b\\x14\\x10\\x16\\x0c\\x0f\\x03\\x11\\x11\\x11\\x03@$FW\\x03\\x15\\x1d\\x03*R\\x03WR\\x03FDELQHW\\x03\\x14\\x032EV\\x03\\x15\\x1d\\x032Q\\x03WKH\\x03FDELQHW\\x03\\x14\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03YDVH\\x03\\x15\\x11\\x03\\x0b+HUH\\x03WKH\\x03DJHQW\\x03JR\\x03WR\\x03FDELQHW\\x03\\x14\\x0f\\x03WKHQ\\x03FDELQHW\\x03\\x15\\x0f\\x03\\x16\\x0f\\x03WKHQ\\x03FRXQWHUWRS\\x03\\x14\\x03DQG\\x03\\x15\\x0c$FW\\x03\\x19\\x1d\\x03*R\\x03WR\\x03FRXQWHUWRS\\x03\\x16\\x032EV\\x03\\x19\\x1d\\x032Q\\x03WKH\\x03FRXQWHUWRS\\x03\\x16\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03DSSOH\\x03\\x14\\x0f\\x03D\\x03EUHDG\\x03\\x14\\x0f\\x03\\x11\\x11\\x11\\x11\\x11\\x11\\x03D\\x03SHSSHU\\x03VKDNHU\\x03\\x14\\x0f\\x03DQG\\x03D\\x03YDVH\\x03\\x16\\x11$FW\\x03\\x1a\\x1d\\x037DNH\\x03SHSSHU\\x03VKDNHU\\x03\\x14\\x03IURP\\x03FRXQWHUWRS\\x03\\x16\\x032EV\\x03\\x1a\\x1d\\x03<RX\\x03SLFN\\x03XS\\x03WKH\\x03SHSSHU\\x03VKDNHU\\x03\\x14\\x03IURP\\x03WKH\\x03FRXQWHUWRS\\x03\\x16\\x11\\x03$FW\\x03\\x1b\\x1d\\x037KLQN>1RZ\\x03,\\x03ILQG\\x03D\\x03SHSSHU\\x03VKDNHU\\x03\\x14\\x11\\x031H[W\\x0f\\x03,\\x03QHHG\\x03WR\\x03SXW\\x03LW\\x03LQ\\x12RQ\\x03GUDZHU\\x03\\x14\\x11@$FW\\x03\\x1c\\x1d\\x03*R\\x03WR\\x03GUDZHU\\x03\\x14\\x032EV\\x03\\x1c\\x1d\\x03'UDZHU\\x03\\x14\\x03LV\\x03FORVHG\\x11\\x03$FW\\x03\\x14\\x13\\x1d\\x032SHQ\\x03GUDZHU\\x03\\x14\\x032EV\\x03\\x14\\x13\\x1d\\x03<RX\\x03RSHQ\\x03'UDZHU\\x03\\x14\\x03ª$FW\\x03\\x14\\x14\\x1d\\x033XW\\x03SHSSHU\\x03VKDNHU\\x03\\x14\\x03LQ\\x12RQ\\x03GUDZHU\\x03\\x14\\x032EV\\x03\\x14\\x14\\x1d\\x03<RX\\x03SXW\\x03SHSSHU\\x03VKDNHU\\x03\\x14\\x03LQ\\x12RQ\\x03WKH\\x03GUDZHU\\x03\\x14\\x11\\x0b\\x14D\\x0c\\x036WDQGDUG$QVZHU\\x1d\\x03L3RG\\x0b\\x14E\\x0c\\x03&R7\\x03\\x0b5HDVRQ\\x032QO\\\\\\x0c7KRXJKW\\x1d\\x03/HW\\nV\\x03WKLQN\\x03VWHS\\x03E\\\\\\x03VWHS\\x11\\x03$SSOH\\x035HPRWH\\x03ZDV\\x03RULJLQDOO\\\\\\x03GHVLJQHG\\x03WR\\x03LQWHUDFW\\x03ZLWK\\x03$SSOH\\x0379\\x11\\x03$SSOH\\x0379\\x03FDQ\\x03EH\\x03FRQWUROOHG\\x03E\\\\\\x03L3KRQH\\x0f\\x03L3DG\\x0f\\x03DQG\\x03L3RG\\x037RXFK\\x11\\x036R\\x03WKH\\x03DQVZHU\\x03LV\\x03L3KRQH\\x0f\\x03L3DG\\x0f\\x03DQG\\x03L3RG\\x037RXFK\\x11$QVZHU\\x1d\\x03L3KRQH\\x0f\\x03L3DG\\x0f\\x03L3RG\\x037RXFK\\x0b\\x14F\\x0c\\x03$FW\\x102QO\\\\\\x03$FW\\x03\\x14\\x1d\\x036HDUFK>$SSOH\\x035HPRWH@\\x032EV\\x03\\x14\\x1d\\x037KH\\x03$SSOH\\x035HPRWH\\x03LV\\x03D\\x03UHPRWH\\x03FRQWURO\\x03ª$FW\\x03\\x15\\x1d\\x036HDUFK>)URQW\\x035RZ@\\x032EV\\x03\\x15\\x1d\\x03&RXOG\\x03QRW\\x03ILQG\\x03>)URQW\\x035RZ@\\x11\\x036LPLODU\\x1d\\x03ª$FW\\x03\\x16\\x1d\\x036HDUFK>)URQW\\x035RZ\\x03\\x0bVRIWZDUH\\x0c@\\x032EV\\x03\\x16\\x1d\\x03)URQW\\x035RZ\\x03LV\\x03D\\x03GLVFRQWLQXHG\\x03PHGLD\\x03FHQWHU\\x03VRIWZDUH\\x03ª$FW\\x03\\x17\\x1d\\x03)LQLVK>\\\\HV@\\x0b\\x14G\\x0c\\x035H$FW\\x03\\x0b5HDVRQ\\x03\\x0e\\x03$FW\\x0c7KRXJKW\\x03\\x14\\x1d\\x03,\\x03QHHG\\x03WR\\x03VHDUFK\\x03$SSOH\\x035HPRWH\\x03DQG\\x03ILQG\\x03WKH\\x03SURJUDP\\x03LW\\x03ZDV\\x03RULJLQDOO\\\\\\x03GHVLJQHG\\x03WR\\x03LQWHUDFW\\x03ZLWK\\x11$FW\\x03\\x14\\x1d\\x036HDUFK>$SSOH\\x035HPRWH@2EV\\x03\\x14\\x1d\\x037KH\\x03$SSOH\\x035HPRWH\\x03LV\\x03D\\x03UHPRWH\\x03FRQWURO\\x03LQWURGXFHG\\x03LQ\\x032FWREHU\\x03\\x15\\x13\\x13\\x18\\x03E\\\\\\x03$SSOH\\x03ª\\x03RULJLQDOO\\\\\\x03GHVLJQHG\\x03WR\\x03FRQWURO\\x03WKH\\x03)URQW\\x035RZ\\x03PHGLD\\x03FHQWHU\\x03SURJUDP\\x03ª7KRXJKW\\x03\\x15\\x1d\\x03$SSOH\\x035HPRWH\\x03ZDV\\x03RULJLQDOO\\\\\\x03GHVLJQHG\\x03WR\\x03FRQWURO\\x03WKH\\x03)URQW\\x035RZ\\x03PHGLD\\x03FHQWHU\\x03SURJUDP\\x11\\x03,\\x03QHHG\\x03WR\\x03VHDUFK\\x03)URQW\\x035RZ\\x03QH[W\\x03DQG\\x03ILQG\\x03ZKDW\\x03RWKHU\\x03GHYLFH\\x03FDQ\\x03FRQWURO\\x03LW\\x11$FW\\x03\\x15\\x1d\\x036HDUFK>)URQW\\x035RZ@2EV\\x03\\x15\\x1d\\x03&RXOG\\x03QRW\\x03ILQG\\x03>)URQW\\x035RZ@\\x11\\x036LPLODU\\x1d\\x03>\\n)URQW\\x035RZ\\x036HDW\\x03WR\\x03(DUWK\\n\\x0f\\x03\\n)URQW\\x035RZ\\x030RWRUVSRUWV\\n\\x0f\\n)URQW\\x035RZ\\x03\\x0bVRIWZDUH\"]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for doc in results['documents']:\n",
    "    print(doc)\n",
    "    print(len(results['documents']))\n",
    "# print(**results['documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LangChain library to build a vector db from the same set of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: Check below Code to integrate Chroma using LangChain\n",
    "# import\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the document and split it into chunks\n",
    "loader = PyPDFLoader(\"data/ReAct.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split it into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ibnabeeali/miniconda3/envs/LangChain/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Documents Returned: 3\n",
      "Published as a conference paper at ICLR 2023\n",
      "We conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks:\n",
      "question answering (HotPotQA, Yang et al., 2018), fact veriﬁcation (Fever, Thorne et al., 2018),\n",
      "text-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao\n",
      "et al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact\n",
      "with, ReAct outperforms vanilla action generation models while being competitive with chain-of-\n",
      "thought reasoning ( CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct\n",
      "andCoT that allows for the use of both internal knowledge and externally obtained information\n",
      "during reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able\n",
      "to outperform imitation or reinforcement learning methods trained with 103∼105task instances,\n",
      "with an absolute improvement of 34% and 10% in success rates respectively. We also demonstrate\n",
      "the importance of sparse, versatile reasoning in decision making by showing consistent advantages\n",
      "over controlled baselines with actions only. Besides general applicability and performance boost,\n",
      "the combination of reasoning and acting also contributes to model interpretability, trustworthiness,\n",
      "and diagnosability across all domains, as humans can readily distinguish information from model’s\n",
      "internal knowledge versus external environments, as well as inspect reasoning traces to understand\n",
      "the decision basis of model actions.\n",
      "To summarize, our key contributions are the following: (1) we introduce ReAct , a novel prompt-\n",
      "based paradigm to synergize reasoning and acting in language models for general task solving; (2) we\n",
      "perform extensive experiments across diverse benchmarks to showcase the advantage of ReAct in a\n",
      "few-shot learning setup over prior approaches that perform either reasoning or action generation in\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# query it\n",
    "query = \"What does it do about hotpotQA and Wikipedia API\"\n",
    "docs = db.similarity_search(query, k=3)\n",
    "print(f\"\"\"Total Documents Returned: {len(docs)}\"\"\")\n",
    "\n",
    "# print results\n",
    "print(docs[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = f\"{'*'*50}\"\n",
    "count = 0\n",
    "similar_docs = db.similarity_search(query, k=3)\n",
    "query_docs = []\n",
    "for doc in similar_docs:\n",
    "    query_docs.append(string)\n",
    "    query_docs.append(f'doc{count+1}')\n",
    "    query_docs.append(doc.page_content)\n",
    "    query_docs.append(string)\n",
    "    count+=1\n",
    "    \n",
    "documents = ''.join(query_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************doc1demonstrating a synergy of reasoning and acting.\n",
      "3.1 S ETUP\n",
      "Domains We consider two datasets challenging knowledge retrieval and reasoning: (1) Hot-\n",
      "PotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning\n",
      "over two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact veriﬁcation\n",
      "benchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based\n",
      "on if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only\n",
      "setup for both tasks, where models only receive the question/claim as input without access to support\n",
      "paragraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with\n",
      "an external environment to support reasoning.\n",
      "Action Space We design a simple Wikipedia web API with three types of actions to support\n",
      "interactive information retrieval: (1) search [entity ], which returns the ﬁrst 5 sentences from\n",
      "the corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the\n",
      "Wikipedia search engine, (2) lookup [string ], which would return the next sentence in the page\n",
      "containing string , simulating Ctrl+F functionality on the browser. (3) finish [answer ], which\n",
      "would ﬁnish the current task with answer . We note that this action space mostly can only retrieve a\n",
      "small part of a passage based on exact passage name, which is signiﬁcantly weaker than state-of-the-\n",
      "art lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,\n",
      "and force models to retrieve via explicit reasoning in language.\n",
      "3.2 M ETHODS\n",
      "ReAct Prompting For HotpotQA and Fever, we randomly select 6 and 3 cases2from the training\n",
      "set and manually compose ReAct -format trajectories to use as few-shot exemplars in the prompts.\n",
      "Similar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense****************************************************************************************************doc2Published as a conference paper at ICLR 2023\n",
      "Type Deﬁnition ReAct CoT\n",
      "SuccessTrue positive Correct reasoning trace and facts 94% 86%\n",
      "False positive Hallucinated reasoning trace or facts 6% 14%\n",
      "FailureReasoning error Wrong reasoning trace (including failing to recover from repetitive steps) 47% 16%\n",
      "Search result error Search return empty or does not contain useful information 23% -\n",
      "Hallucination Hallucinated reasoning trace or facts 0% 56%\n",
      "Label ambiguity Right prediction but did not match the label precisely 29% 28%\n",
      "Table 2: Types of success and failure modes of ReAct andCoT on HotpotQA, as well as their\n",
      "percentages in randomly selected examples studied by human.\n",
      "ReAct vs.CoT On the other hand, ReAct outperforms CoT on Fever (60.9 vs. 56.3) and slightly\n",
      "lags behind CoT on HotpotQA (27.4 vs. 29.4). Fever claims for SUPPORTS/REFUTES might only\n",
      "differ by a slight amount (see Appendix D.1), so acting to retrieve accurate and up-to-date knowledge\n",
      "is vital. To better understand the behavioral difference between ReAct andCoT on HotpotQA, we\n",
      "randomly sampled 50 trajectories with correct and incorrect answers (judged by EM) from ReAct\n",
      "andCoT respectively (thus 200 examples in total), and manually labeled their success and failure\n",
      "modes in Table 2. Some key observations are as follows:\n",
      "A)Hallucination is a serious problem for CoT, resulting in much higher false positive rate than\n",
      "ReAct (14% vs. 6%) in success mode, and make up its major failure mode (56%). In contrast, the\n",
      "problem solving trajectory of ReAct is more grounded, fact-driven, and trustworthy, thanks to the\n",
      "access of an external knowledge base.\n",
      "B)While interleaving reasoning, action and observation steps improves ReAct ’s grounded-\n",
      "ness and trustworthiness, such a structural constraint also reduces its ﬂexibility in formulating\n",
      "reasoning steps , leading to more reasoning error rate than CoT. we note that there is one frequent****************************************************************************************************doc3Published as a conference paper at ICLR 2023\n",
      "We conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks:\n",
      "question answering (HotPotQA, Yang et al., 2018), fact veriﬁcation (Fever, Thorne et al., 2018),\n",
      "text-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao\n",
      "et al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact\n",
      "with, ReAct outperforms vanilla action generation models while being competitive with chain-of-\n",
      "thought reasoning ( CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct\n",
      "andCoT that allows for the use of both internal knowledge and externally obtained information\n",
      "during reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able\n",
      "to outperform imitation or reinforcement learning methods trained with 103∼105task instances,\n",
      "with an absolute improvement of 34% and 10% in success rates respectively. We also demonstrate\n",
      "the importance of sparse, versatile reasoning in decision making by showing consistent advantages\n",
      "over controlled baselines with actions only. Besides general applicability and performance boost,\n",
      "the combination of reasoning and acting also contributes to model interpretability, trustworthiness,\n",
      "and diagnosability across all domains, as humans can readily distinguish information from model’s\n",
      "internal knowledge versus external environments, as well as inspect reasoning traces to understand\n",
      "the decision basis of model actions.\n",
      "To summarize, our key contributions are the following: (1) we introduce ReAct , a novel prompt-\n",
      "based paradigm to synergize reasoning and acting in language models for general task solving; (2) we\n",
      "perform extensive experiments across diverse benchmarks to showcase the advantage of ReAct in a\n",
      "few-shot learning setup over prior approaches that perform either reasoning or action generation in**************************************************\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate an LLM via API to test the qualitative performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_TOKEN}\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(query):\n",
    "    \n",
    "    string = f\"{'*'*50}\"\n",
    "    count = 0\n",
    "    similar_docs = db.similarity_search(query, k=3)\n",
    "    query_docs = []\n",
    "    for doc in similar_docs:\n",
    "        query_docs.append(string)\n",
    "        query_docs.append(f'doc{count+1}')\n",
    "        query_docs.append(doc.page_content)\n",
    "        query_docs.append(string)\n",
    "        count+=1\n",
    "        \n",
    "    documents = ''.join(query_docs)\n",
    "\n",
    "    prompt = f\"\"\"Given the user query: '{query}', and the relevant document chunks: {documents},\n",
    "      your task is to generate an informative response based solely on the information contained within the \n",
    "      provided document chunks.\"\"\"\n",
    "    \n",
    "    output = query({\n",
    "\t\"inputs\": prompt,\n",
    "    })\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 2\u001b[0m\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mprompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhen was this paper published and at which conference\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[80], line 20\u001b[0m, in \u001b[0;36mprompt\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     14\u001b[0m     documents \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(query_docs)\n\u001b[1;32m     16\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mGiven the user query: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, and the relevant document chunks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocuments\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m      your task is to generate an informative response based solely on the information contained within the \u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124m      provided document chunks.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m\t\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "result = prompt(\"When was this paper published and at which conference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
