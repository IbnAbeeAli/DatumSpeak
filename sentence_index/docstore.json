{"docstore/metadata": {"d8141fb3-ab68-474a-9a84-2b5e334b1978": {"doc_hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669"}, "16795457-88d0-43b4-a2a0-816bd0398062": {"doc_hash": "9a53890b3b81bdb886cce4db0612fb05077c52ccdf6945a8fe38c364f4f6046c", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "16863c14-7fa4-40f2-9675-f6d1f5e73b0f": {"doc_hash": "5a120c59d195918266928378e24b8c9984b773955606943a4e7d861aa9318585", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "ed2edda2-6d00-4d88-b6ba-b3c38f530584": {"doc_hash": "e1e25b63b685526cc2c2072f3bbd0b746e70f314250d6d34ff2343dde48785fd", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "de15932a-26f7-4142-87a9-4438ba6eeae6": {"doc_hash": "a2e0e752224cb7b7882d1222e6e61dad2a82616444db481fe27f65714b646c95", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "f9c1e995-6d7d-43ba-a1ba-c5344007638d": {"doc_hash": "cba4f763c6e67a55ffa1fceae5c1d8156fe1ed28c9cc2f4e2a429cab7e8eb88a", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "26f4fed3-48f4-43bf-b791-3eb09d24ed5f": {"doc_hash": "f8ae78727d4050dda21f54d4941227895e419cdb6b6b5fe0f8252ed25c3f7633", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "19d730d0-825a-4439-885a-dbd50d1bc147": {"doc_hash": "6adecc8eb0a7005c9d684de599eca3258a4b9183c5cc081ed6e4480d899b5195", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "b49ce16f-348c-4cd1-8cb6-9dcc74e7ae4c": {"doc_hash": "9443259cdd3d3922869a2ba3a424ae96c74a150b7e78d9c99c5339838044f48b", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "e544e3f6-6ab6-4f5d-a2b2-b98da9651da5": {"doc_hash": "639dd20eb6b4a5599ae663ab86674e2cf04f95c813929fe746403a8e0e0ebef0", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "a9afb300-8619-430f-9ff8-06ed71a402be": {"doc_hash": "5e6ceee2799f967f71fc9218ac0e66a51373c9a64fe68854b991fb08eadf5f98", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "b261699b-4d9a-4888-888b-696381fca41a": {"doc_hash": "f62a02104b3f9c33e29ef1e1109b505554b629ff75faca2b596126b1e388205f", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "6580de09-1c7e-4d29-87d2-f4a62f8b9f05": {"doc_hash": "743ac75b22ffb5cbbe385b2356915bcaa15bd786710f4dcc41331afe4edeb170", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "844cd9dc-85f5-4e67-8617-80481d075dc9": {"doc_hash": "7441bb163f7b786aa900d14b2e037b582348607c93b546f85d09bf9a358781f0", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "5a5dea97-9e72-4d47-851d-51d7d912346a": {"doc_hash": "65f1cc628518348450810ad0642b80457082c738f5522870d65873db7239f177", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "be73ca2b-08ae-4ae5-a08b-5f503f529c49": {"doc_hash": "37fe86e259f24f913b633e18706fd4b65106185aa5a27e177488febd77b588c6", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "f93a121e-470a-40bb-abb9-967d52be7211": {"doc_hash": "d2d747a5f5888af7ea98dcfd25487b4799580c185b58f71471c0b21303629311", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "0aee29e8-1b66-41f9-ac12-9d47a48e9455": {"doc_hash": "e8817c972383b47dce3473bc142b0e794d24fd51d7122032d72396a04f35f437", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "31ec87f2-0d33-4194-922f-c094e260481a": {"doc_hash": "e66f42a17a1012541baed21e935b187e9c1041ca442616f6970048831530355e", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "9fd5bcc5-308b-4b1e-94f7-a3c19d73d366": {"doc_hash": "701303db6caad0b6077fc31c31dbfddd6ad63a87c5c6e797169d0ad583235956", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "12955511-a656-4d76-b211-062c0c679438": {"doc_hash": "a698dbe135278d73bfd44e1497cfe99478c68168fd30af752cfc6de21116ea6b", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "e3f4f903-820e-4236-8012-b6418af40470": {"doc_hash": "82322e6bc79b1f68c88bb598464f5c324f37b4cc07866c2a4763b38a8cd5b6b4", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "e37ca8f1-f3e3-4f94-ba1c-ca5f8ce6dc59": {"doc_hash": "b177afa73cc06d3e0fbcefee96a21349569218daca2e45fb709560b6aa731034", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "5203e67d-96a2-45d0-ad6f-09d31ee2e4ab": {"doc_hash": "0c0d5ea9730b3fc4c20acc78dfd9e5ceeb137a763ff329b1c0c702c6cb894d94", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "5744f482-e40e-49e1-9e90-dee0c9d702b6": {"doc_hash": "60c57540bcded5a91435c7a322e53eafcdcbe60b1633e45832a59f09db4ec521", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "e8c0879f-22ef-4b74-9485-fc84e0be0b5d": {"doc_hash": "8d530974cc8865dc44af4ba8a938f40013b8aadc1ade4e35901725c63926a567", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}, "1d05689a-2158-495f-a8fb-050b7cb346ed": {"doc_hash": "7bb349382e750065d4dd1709d9208e1eca63e2f5f6237aa9201ca2911a0c9c54", "ref_doc_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978"}}, "docstore/data": {"16795457-88d0-43b4-a2a0-816bd0398062": {"__data__": {"id_": "16795457-88d0-43b4-a2a0-816bd0398062", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16863c14-7fa4-40f2-9675-f6d1f5e73b0f", "node_type": "1", "metadata": {}, "hash": "5a120c59d195918266928378e24b8c9984b773955606943a4e7d861aa9318585", "class_name": "RelatedNodeInfo"}}, "text": "MIND2W EB: Towards a Generalist Agent for the Web\nXiang Deng\u2217Yu Gu Boyuan Zheng Shijie Chen\nSamuel Stevens Boshi Wang Huan Sun\u2217Yu Su\u2217\nThe Ohio State University\nhttps://osu-nlp-group.github.io/Mind2Web\nAbstract\nWe introduce MIND2W EB, the first dataset for developing and evaluating generalist\nagents for the web that can follow language instructions to complete complex tasks\non any website. Existing datasets for web agents either use simulated websites or\nonly cover a limited set of websites and tasks, thus not suitable for generalist web\nagents. With over 2,000open-ended tasks collected from 137websites spanning\n31domains and crowdsourced action sequences for the tasks, MIND2W EBpro-\nvides three necessary ingredients for building generalist web agents: 1) diverse\ndomains, websites, and tasks, 2) use of real-world websites instead of simulated\nand simplified ones, and 3) a broad spectrum of user interaction patterns. Based\nonMIND2W EB, we conduct an initial exploration of using large language models\n(LLMs) for building generalist web agents. While the raw HTML of real-world web-\nsites are often too large to be fed to LLMs, we show that first filtering it with a small\nLM significantly improves the effectiveness and efficiency of LLMs. Our solution\ndemonstrates a decent level of performance, even on websites or entire domains the\nmodel has never seen before, but there is still a substantial room to improve towards\ntruly generalizable agents. We open-source our dataset, model implementation,\nand trained models ( https://osu-nlp-group.github.io/Mind2Web ) to facilitate\nfurther research on building a generalist agent for the web.\n1 Introduction\nThe web now hosts billions of websites [ 7] that cover virtually every aspect of the digital world. In\nthis work, we seek to answer the question: How can we build a generalist agent for the web that,\ngiven any website, can follow language instructions and carry out the corresponding tasks? Some\nexemplar tasks for such an agent are shown in Figure 1. A generalist agent could make the web more\naccessible, which is becoming increasingly difficult as modern websites provide increasingly more\nfunctionalities that also increase their complexity and learning curve. On the other hand, such an agent\nmay also turn the entire web into an unprecedentedly powerful and versatile tool [ 23,27] that can\nenhance large language models (LLMs). For example, it may be used as a plugin for ChatGPT [ 24] to\ndirectly acquire information and carry out actions on HTML websites, instead of only retrieving web\ncontent through a retriever tool [ 8,18] or relying on pre-defined APIs for each web service [ 34,37].\nA generalist agent for the web shall meet the following desiderata: First, it shall work on any website\non the Internet . Since it is infeasible to collect sufficient training data that covers all websites, this\nrequires the agent to be inherently generalizable to websites or even domains it has never seen before.\nSecond, it shall work on real-world websites, which can be dynamic, complex, and noisy . Most\nmodern websites are dynamic, generating and rendering different content in response to user actions.\nThis necessitates the agent to model each website as a partially-observable environment instead of\nassuming full knowledge a priori . The agent should also not make strong simplifying assumptions\n\u2217Corresponding authors: {deng.595, sun.397, su.809}@osu.edu\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.arXiv:2306.06070v3  [cs.CL]  9 Dec 2023\n\n(a) Find one-way flights from New York to \nToronto. (b) Book a roundtrip on July 1 from Mumbai to \nLondon and vice versa on July 5 for two adults. \n(c) Find a flight from Chicago to London on \n20 April and return on 23 April. \n(d) Find Elon Musk's profile and follow, start \nnotifications and like the latest tweet. \n(e) Browse comedy films streaming on Netflix \nthat was released from 1992 to 2007. \n(f) Open page to schedule an appointment for \ncar knowledge test.", "start_char_idx": 0, "end_char_idx": 4044, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16863c14-7fa4-40f2-9675-f6d1f5e73b0f": {"__data__": {"id_": "16863c14-7fa4-40f2-9675-f6d1f5e73b0f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16795457-88d0-43b4-a2a0-816bd0398062", "node_type": "1", "metadata": {}, "hash": "9a53890b3b81bdb886cce4db0612fb05077c52ccdf6945a8fe38c364f4f6046c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed2edda2-6d00-4d88-b6ba-b3c38f530584", "node_type": "1", "metadata": {}, "hash": "e1e25b63b685526cc2c2072f3bbd0b746e70f314250d6d34ff2343dde48785fd", "class_name": "RelatedNodeInfo"}}, "text": "(b) Book a roundtrip on July 1 from Mumbai to \nLondon and vice versa on July 5 for two adults. \n(c) Find a flight from Chicago to London on \n20 April and return on 23 April. \n(d) Find Elon Musk's profile and follow, start \nnotifications and like the latest tweet. \n(e) Browse comedy films streaming on Netflix \nthat was released from 1992 to 2007. \n(f) Open page to schedule an appointment for \ncar knowledge test. \n3.1% 5.8% (27.4%) \n6.2% \n4.3% \n3.7% 2.4% 1.8% 5.0% \n3.3% 2.3% \n1.6% 1.6% \n3.8% 3.5% 3.5% 3.2% \n2.0% 3.7% 3.5% 6.5% \n3.4% \n2.1% 1.8% 1.7% 1.9% 2.0% 2.7% 3.5% 2.8% Travel (27.4%) Shopping (17.5%) \nEntertainment (16.1%) Info (20.4%) Service (18.3%) \nOther \n6.2% \nGround \n3.7% Airlines \n5.8% \nRestaurant \n4.3% General \n3.1% \nCar\nRental \n2.4% Hotel \n1.8% Job \n3.5% \nEducation \n2.8% \nFinance \n2.7% Cook- \ning \n2.9% Housing \n3.7% \nSocial \nMedia \n3.5% \nWeathe r \n1.9% Government \n3.4% \nPet \n2.1% Health \n6.5% \nShipp-\ning \n1.8% Moving \n1.7% Home \nService \n2.5% Speciality \n5.0% Auto \n3.4% \nGeneral \n3.3% Digital \n2.3% \nDepar- \ntment \n1.6% Fashion  \n1.6% \nEvent \n3.8% Music \n3.5% Sports \n3.5% Movie \n3.2% \nGame \n2.0% Figure 1: Sample tasks and all domains featured in MIND2W EB. The array of diversity allows for\ntesting an agent\u2019s generalizability across tasks on the same website (a vs. b), similar tasks on different\nwebsites (a vs. c), and even to entirely disparate tasks, websites, and domains (d \u2212f).\nabout the environments but must embrace the full complexity and sometimes noise, e.g., due to\nsub-optimal website designs. Finally, it shall support diverse and sophisticated interactions with\nwebsites. Tasks from users can be highly diverse and take a large number of steps to complete ( e.g.,\nthe task in Figure 1(b) would take 14actions). An agent that only supports simple tasks may provide\nlimited value to users.\nBuilding an agent for the web is not entirely new. There have been numerous prior efforts in\nvaried forms. However, none of them meets all the requirements for a generalist agent listed above.\nExisting work falls short in one to all of the following aspects: 1) Only operating in a limited and\npre-specified set of websites [ 5,21,22,35,40], 2) making strong simplifying assumptions about the\nwebsites [ 22,40], and 3) only supporting specific types of tasks [ 21,22,40] and/or requiring tedious\nstep-by-step instructions from users [ 5,21,22,39]. Meanwhile, LLMs have been shown to excel at\ngrounded language understanding in complex environments with good generalizability and sample\nefficiency [ 2,13,17,33]. It is therefore promising to explore LLMs as a candidate solution towards\ngeneralist agents for the web. However, there lacks a good dataset that can support the development\nand evaluation of generalist web agents, which is the focus of this work.\nIn light of this, we present MIND2W EB, a new dataset with natural language tasks and manually\nannotated action sequences for developing and evaluating generalist agents for the web. It offers the\nfollowing unique features:\n1. Diverse coverage of domains, websites, and tasks. MIND2W EBboasts a collection of over 2,000\ntasks curated from 137websites that span 31different domains.", "start_char_idx": 3630, "end_char_idx": 6804, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed2edda2-6d00-4d88-b6ba-b3c38f530584": {"__data__": {"id_": "ed2edda2-6d00-4d88-b6ba-b3c38f530584", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16863c14-7fa4-40f2-9675-f6d1f5e73b0f", "node_type": "1", "metadata": {}, "hash": "5a120c59d195918266928378e24b8c9984b773955606943a4e7d861aa9318585", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de15932a-26f7-4142-87a9-4438ba6eeae6", "node_type": "1", "metadata": {}, "hash": "a2e0e752224cb7b7882d1222e6e61dad2a82616444db481fe27f65714b646c95", "class_name": "RelatedNodeInfo"}}, "text": "Meanwhile, LLMs have been shown to excel at\ngrounded language understanding in complex environments with good generalizability and sample\nefficiency [ 2,13,17,33]. It is therefore promising to explore LLMs as a candidate solution towards\ngeneralist agents for the web. However, there lacks a good dataset that can support the development\nand evaluation of generalist web agents, which is the focus of this work.\nIn light of this, we present MIND2W EB, a new dataset with natural language tasks and manually\nannotated action sequences for developing and evaluating generalist agents for the web. It offers the\nfollowing unique features:\n1. Diverse coverage of domains, websites, and tasks. MIND2W EBboasts a collection of over 2,000\ntasks curated from 137websites that span 31different domains. This extensive range of tasks and\ndomains not only provides a vast landscape for exploration and learning but also opens up a new level\nof versatility and complexity, fostering a more comprehensive evaluation of generalist web agents.\n2\n\n2. Use of real-world websites. MIND2W EBreplaces the oversimplified simulation environments\ncommonly found in other datasets with an authentic, vibrant, and unpredictable realm of real-world\nwebsites. We provide full traces of user interactions, webpage snapshots, and network traffic, making\nit a rich source of raw, unfiltered, and dynamic data. By doing so, MIND2W EBequips models with\nthe capacity to interact and cope with the complexities and uncertainties of real-world environments,\nthereby encouraging the development of more robust and adaptive models.\n3. A broad spectrum of user interaction patterns. MIND2W EBenables users to engage in\nsophisticated ways with websites, as opposed to basic operations such as searching, following links,\nand reading content commonly found in existing work. Users can click, select, and type in any\nelements on the website, which significantly expands the space of possible tasks. This captures all the\ncommon actions users do on websites in real life and promotes the development of agents capable of\nhandling complex tasks.\nWith the diverse domains and websites, we create challenging out-of-distribution evaluation settings\nwhere agents are tested on their generalizability to websites or even entire domains never seen during\ntraining. This presents a representative evaluation of generalist agents working on unseen websites.\nMIND2W EBenables us to conduct an initial exploration in using LLMs for building generalist web\nagents. The HTML document of real-world webpages may contain thousands of elements, which\nmakes it infeasible or too costly to be fed into an LLM\u2019s context. To address this issue, we propose\nMINDACT, a two-stage model that involves first using a fine-tuned small LM to filter the web elements\nand then using an LLM to select from the filtered elements in a multi-choice question answering\nfashion and predict the corresponding action with the selected element. We show that MINDACT\nsignificantly outperforms modeling strategies commonly adopted by prior work and achieves a decent\nlevel of generalization. It can also work well with both open-source LLMs like Flan-T5 [ 10] through\nfine-tuning and closed-source LLMs like GPT-3.5-turbo and GPT-4 through in-context learning.\nHowever, there is still substantial room for further improvement towards generalist agents for the web.\nPromising future directions include integrating multi-modal information, reinforcement learning with\nfeedback from real websites, and specialized LMs for web understanding and action taking.\n2 M IND2W EBDataset\nUnlike existing datasets predominantly constructed within simulated environments [ 31,40], our\nobjective is to bridge the gap between simulation and reality so that agents trained on our dataset can\nwork on real-world websites out of the box. To achieve this, our approach for data collection adheres\nto the following principles. Firstly, instead of recreating websites in simulation, which often leads to\noversimplified environments, we engage directly with real-world websites and capture snapshots of\nthese environments. Secondly, we collate a diverse set of websites from varied domains and crowd-\nsource realistic tasks that cover a wide range of functionalities provided by these websites. Finally,\nacknowledging the challenge of perfectly replicating the complexity of real-world environments, we\nstrive to capture a comprehensive snapshot of each website and the full interaction trace, to the extent\nthat all the tasks can be seamlessly replayed offline. This supports rich modeling and evaluation\napproaches, ensuring a robust and practical dataset for research.\n2.1 Task Definition\nThe primary objective of MIND2W EBis for the agent to complete a specific task on the target website\nthrough a series of actions.", "start_char_idx": 6011, "end_char_idx": 10827, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de15932a-26f7-4142-87a9-4438ba6eeae6": {"__data__": {"id_": "de15932a-26f7-4142-87a9-4438ba6eeae6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed2edda2-6d00-4d88-b6ba-b3c38f530584", "node_type": "1", "metadata": {}, "hash": "e1e25b63b685526cc2c2072f3bbd0b746e70f314250d6d34ff2343dde48785fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9c1e995-6d7d-43ba-a1ba-c5344007638d", "node_type": "1", "metadata": {}, "hash": "cba4f763c6e67a55ffa1fceae5c1d8156fe1ed28c9cc2f4e2a429cab7e8eb88a", "class_name": "RelatedNodeInfo"}}, "text": "To achieve this, our approach for data collection adheres\nto the following principles. Firstly, instead of recreating websites in simulation, which often leads to\noversimplified environments, we engage directly with real-world websites and capture snapshots of\nthese environments. Secondly, we collate a diverse set of websites from varied domains and crowd-\nsource realistic tasks that cover a wide range of functionalities provided by these websites. Finally,\nacknowledging the challenge of perfectly replicating the complexity of real-world environments, we\nstrive to capture a comprehensive snapshot of each website and the full interaction trace, to the extent\nthat all the tasks can be seamlessly replayed offline. This supports rich modeling and evaluation\napproaches, ensuring a robust and practical dataset for research.\n2.1 Task Definition\nThe primary objective of MIND2W EBis for the agent to complete a specific task on the target website\nthrough a series of actions. Each instance in our dataset contains three components:\nTask description, which outlines the high-level goal of the task. We intentionally avoid low-level,\nstep-by-step instructions, aiming to foster the development of agents that can comprehend and carry\nout tasks in a more autonomous fashion, rather than merely following prescriptive directives.\nAction sequence , which is the sequence of actions required to accomplish the task on the website.\nEach action in the sequence comprises a (Target Element, Operation) pair. The Target\nElement is an interactable element on the current webpage, and the Operation refers to the action\nto be executed on that element. We support three common operations: Click (also including Hover\nandPress Enter ),Type , and Select Option . For Type andSelect Option , they also require\nan additional value as an argument. Actions in a sequence often span multiple webpages of a website.\n3\n\nTask Description:Show me the reviews for the auto repair business closest to 10002.ActionSequence:Target ElementOperation1.[searchbox] FindTYPE: auto repair2.[button] Auto RepairCLICK3.[textbox] NearTYPE:100024.[button] 10002CLICK5.[button] SearchCLICK6.[switch] Show BBB Accredited onlyCLICK7.[svg]CLICK8.[button] Sort ByCLICK9.[link] Fast Lane 24 Hour Auto RepairCLICK10.[link] Read ReviewsCLICKWebpage Snapshots:\n<button>Show BBB Accredited only</button><em>Auto Repair</em><button>Search</button>\nAction 1Action 2\n<input name=\"find_text\" type=\"search\">Action 5\nAction 6\nAction 9\n<span>Fast Lane 24 Hour Auto Repair</span>\nAction 10\n<a href=\"link:XXX\">Read Reviews</a>Figure 2: A sample data instance of our dataset with the three components. Actions marked in red\nwill result in a transition to a new webpage.\nWebpage snapshots , which constitute the environment within which the task is performed. We\nprovide the snapshots in a variety of formats to accommodate different modeling approaches: self-\ncontained MHTML file that includes the raw HTML code of the webpage, DOM snapshot containing\nthe DOM tree along with the layout and style information of the screenshot of the rendered webpage,\nHAR file that includes all the network traffic for replaying the interaction if needed, and trace file that\ncomprises the complete interaction trace during the task annotation process.\nThe agent receives the task description in the beginning. At each step, it also receives the current\nwebpage and the history of previous actions. The objective is to accurately predict the subsequent\naction, which encompasses the target element for interaction and the operation.\n2.2 Data Collection\nOur data collection process consists of four stages: website selection, task proposal, task demonstra-\ntion, and task verification. Website selection and task verification are done by the authors. For task\nproposal and demonstration, we develop a sophisticated annotation tool using Playwright2and hire\nannotators through Amazon Mechanical Turk. Refer to Supplementary for annotation tool details.\nWebsite Selection. We start with 5top-level domains: Travel, Shopping, Service, Entertainment, and\nInformation, which are subsequently broken down into 31(secondary) domains. We select websites\nwithin each domain based on their popularity in the US, as ranked by similarweb.com. We manually\nselect 3-5representative websites per domain, resulting in a collection of 137websites in total.\nTask Proposal. We present the annotators with a target website, a concise description of the website,\nand a few sample tasks associated with it. The annotators are then asked to propose open-ended\nand realistic tasks based on three criteria: the tasks should be of diverse types, require multiple\nrounds of interaction, and describe the high-level goal instead of step-by-step instructions.", "start_char_idx": 9848, "end_char_idx": 14608, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9c1e995-6d7d-43ba-a1ba-c5344007638d": {"__data__": {"id_": "f9c1e995-6d7d-43ba-a1ba-c5344007638d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de15932a-26f7-4142-87a9-4438ba6eeae6", "node_type": "1", "metadata": {}, "hash": "a2e0e752224cb7b7882d1222e6e61dad2a82616444db481fe27f65714b646c95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26f4fed3-48f4-43bf-b791-3eb09d24ed5f", "node_type": "1", "metadata": {}, "hash": "f8ae78727d4050dda21f54d4941227895e419cdb6b6b5fe0f8252ed25c3f7633", "class_name": "RelatedNodeInfo"}}, "text": "Refer to Supplementary for annotation tool details.\nWebsite Selection. We start with 5top-level domains: Travel, Shopping, Service, Entertainment, and\nInformation, which are subsequently broken down into 31(secondary) domains. We select websites\nwithin each domain based on their popularity in the US, as ranked by similarweb.com. We manually\nselect 3-5representative websites per domain, resulting in a collection of 137websites in total.\nTask Proposal. We present the annotators with a target website, a concise description of the website,\nand a few sample tasks associated with it. The annotators are then asked to propose open-ended\nand realistic tasks based on three criteria: the tasks should be of diverse types, require multiple\nrounds of interaction, and describe the high-level goal instead of step-by-step instructions. To further\nstimulate creativity and boost diversity, we use ChatGPT to generate seed tasks by prompting it\nto test different functionalities of a website. We generate 50seed tasks per website, of which 10\nare randomly sampled and presented to the annotator each time. These seed tasks are mainly for\ninspiration\u2014annotators are explicitly instructed not to directly use them and we reject task proposals\nthat are highly similar to the seed tasks. All the proposed tasks are further screened by the authors to\nensure quality and diversity before entering the demonstration phase.\nTask Demonstration. We develop a Playwright-based tool for demonstration (Figure 2). Workers\nwill use the tool to demonstrate how to perform the tasks they have proposed within a web browser.\nTo ensure accuracy, each interaction round is split into two parts: element selection andoperation\nselection . At each step, the worker first selects an element on the webpage by clicking within the\nbrowser. They are then asked to confirm the selection and choose the operation to execute on the\nselected element. Once the task is completed, the worker is given another opportunity to review and\nmodify the task description.\n2https://playwright.dev/\n4\n\nTable 1: Statistics of M IND2W EBcompared with existing datasets.\n# Dom. # Env. Env. TypeAvg. #\nElements# Tasks Task Info.Avg. #\nActions\nMiniWoB++ [22] \u2212 100Simplified\nmobile websites28 100 Low-level 3.6\nWebShop [40] 1 1Simplified\nshopping websites38 12,000products High-level 11.3\nRUSS [39] \u2212 22Real-world\nwebsites801 80 High & low 5.4\nPixelHelp [21] 4 4 Mobile apps \u2212 187 High & low -\nMETA-GUI [35] 6 11 Mobile apps 79 1,125dialogues High-level 4.3\nMoTIF [5] 15 125 Mobile apps 188 756 High & Low 4.4\nMIND2W EB 5/31 137Real-world\nwebsites1,135 2,350 High-level 7.3\nTask Verification. Lastly, all task demonstrations are verified by the authors to ensure the following:\nFirst, all actions are accurately reflected in the task description. The authors will modify the task\ndescription if needed to align it with the annotated actions; Second, the recorded actions are correct\nand clean, with extraneous steps discarded. Finally, the starting and ending points of tasks are\nconsistent, such as excluding actions for closing popup windoes, or ending the annotation at the\nsearch result page if the task was to find a certain item without clicking on specific items. After\nverification, we discarded 61out of the total 2,411tasks. Among the 2,350retained tasks, the task\ndescription was refined in 390instances to better correspond with the demonstrated actions, while\nsome extraneous steps were discarded in 187instances. Overall, the data collection pipeline has been\nproven effective and produces high-quality data.\n2.3 Comparison with Existing Work and Research Challenges\nMIND2W EBpresents a unique ensemble of research challenges for the development of generalist\nagents for the web in real-world settings. As shown in Table 1, MIND2W EBdistinguishes itself\nfrom existing literature in several ways. Firstly, MIND2W EBspans across 137websites from\n31domains, allowing comprehensive testing of an agent\u2019s ability in generalizing across varied\nenvironments. Secondly, we utilize real-world websites without manual simplification. Consequently,\nthe included environments exhibit complexity far surpassing that encountered in previous studies,\nyet better reflecting the intricacy of the modern web.", "start_char_idx": 13778, "end_char_idx": 18029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26f4fed3-48f4-43bf-b791-3eb09d24ed5f": {"__data__": {"id_": "26f4fed3-48f4-43bf-b791-3eb09d24ed5f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9c1e995-6d7d-43ba-a1ba-c5344007638d", "node_type": "1", "metadata": {}, "hash": "cba4f763c6e67a55ffa1fceae5c1d8156fe1ed28c9cc2f4e2a429cab7e8eb88a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19d730d0-825a-4439-885a-dbd50d1bc147", "node_type": "1", "metadata": {}, "hash": "6adecc8eb0a7005c9d684de599eca3258a4b9183c5cc081ed6e4480d899b5195", "class_name": "RelatedNodeInfo"}}, "text": "Overall, the data collection pipeline has been\nproven effective and produces high-quality data.\n2.3 Comparison with Existing Work and Research Challenges\nMIND2W EBpresents a unique ensemble of research challenges for the development of generalist\nagents for the web in real-world settings. As shown in Table 1, MIND2W EBdistinguishes itself\nfrom existing literature in several ways. Firstly, MIND2W EBspans across 137websites from\n31domains, allowing comprehensive testing of an agent\u2019s ability in generalizing across varied\nenvironments. Secondly, we utilize real-world websites without manual simplification. Consequently,\nthe included environments exhibit complexity far surpassing that encountered in previous studies,\nyet better reflecting the intricacy of the modern web. With an average of over 1,000elements per\npage embedded within complex DOM structures, how to effectively process such long and highly\nstructured documents presents a significant challenge for modeling. Lastly, we direct the annotators\nto propose open-ended tasks that explore different functionalities of the website to mimic genuine\nweb usage. Meanwhile, contrary to prior studies [ 5,21,22,39] that provide step-by-step directives\nand primarily focus on testing the agent\u2019s ability to translate low-level instructions into actions, e.g.,\n\u201cType New York in the location field, click the search button and choose the tomorrow tab ,\u201d we opted\nfor the setting where only high-level goals are available, e.g., \u201cWhat is the weather for New York\ntomorrow ?\u201d This poses a much greater yet realistic planning and grounding challenge for the agent.\n3 Method: M INDACT\nEmploying the data from MIND2W EB, we introduce an exploratory framework, MINDACT, for our\ntask, leveraging the power of LLMs. Raw HTML documents, which could consist of thousands\nof elements, are either infeasible or cost-prohibitive to be directly fed into LLMs. We propose a\ntwo-stage process that synergizes the strength of small and large LMs, as shown in Figure 3. In the\nfirst stage, a fine-tuned small LM is used to rank the elements present on a webpage, yielding a small\npool of promising candidates. In the second stage, these candidate elements are consolidated to form\na representative snippet of the webpage, which is then processed by an LLM to predict the final\naction, including predicting both the element for interaction and the corresponding operation.\n3.1 Candidate Generation with Small LMs\nGiven the task description, the snapshot of the webpage at step t, and the actions performed in the\npreceding t\u22121steps, we treat candidate generation as a ranking task. The task is to select the top- k\n5\n\nHTML \nDocumentRanking LM\n+ Target \nElementOperationPrediction LLM\u2026Candidate \nElementsTop-k \nElementsHTML \nSnippet\nTask \nDescriptionPrevious \nActions\nCandidate Representation\nancestors: /html/div dialog/ul location \nsearch results\ntarget: (button id=5 (span (span \nBoston ) (span NY, USA ) ) )\nTask Query\nTask : Check for pickup restaurant \navailable in Boston, NY on March 18, \n5pm with just one guest\nPrevious Actions:\n[combobox] Reservation type -> \nSELECT: Pickup\n[svg] -> CLICK \n[searchbox] Find a location -> TYPE: \nBostonSmall LMClassifier[0, 1]\nScoreFigure 3: The overall pipeline for MINDACTwith\na small ranking LM for candidate generation, and a\nlarge prediction LM for action prediction.\nHTML \nDocumentRanking LM\n+ Target\nElementOperationPrediction LLM\u2026Candidate \nElementsTop-k \nElementsHTML \nSnippet\nTask \nDescriptionPrevious \nActions\nCandidate Representation\nancestors: /html/div dialog/ul location \nsearch results\ntarget: (button id=5 (span (span \nBoston ) (span NY, USA ) ) )\nTask Query\nTask : Check for pickup restaurant \navailable in Boston, NY on March 18, \n5pm with just one guest\nPrevious Actions:\n[combobox] Reservation type -> \nSELECT: Pickup\n[svg] -> CLICK \n[searchbox] Find a location -> TYPE: \nBostonRanking LMClassifier[0, 1]\nScoreFigure 4: Illustration of the candidate gener-\nation module and the templates for construct-\ning task query and candidate representation.", "start_char_idx": 17252, "end_char_idx": 21302, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19d730d0-825a-4439-885a-dbd50d1bc147": {"__data__": {"id_": "19d730d0-825a-4439-885a-dbd50d1bc147", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26f4fed3-48f4-43bf-b791-3eb09d24ed5f", "node_type": "1", "metadata": {}, "hash": "f8ae78727d4050dda21f54d4941227895e419cdb6b6b5fe0f8252ed25c3f7633", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b49ce16f-348c-4cd1-8cb6-9dcc74e7ae4c", "node_type": "1", "metadata": {}, "hash": "9443259cdd3d3922869a2ba3a424ae96c74a150b7e78d9c99c5339838044f48b", "class_name": "RelatedNodeInfo"}}, "text": "HTML \nDocumentRanking LM\n+ Target\nElementOperationPrediction LLM\u2026Candidate \nElementsTop-k \nElementsHTML \nSnippet\nTask \nDescriptionPrevious \nActions\nCandidate Representation\nancestors: /html/div dialog/ul location \nsearch results\ntarget: (button id=5 (span (span \nBoston ) (span NY, USA ) ) )\nTask Query\nTask : Check for pickup restaurant \navailable in Boston, NY on March 18, \n5pm with just one guest\nPrevious Actions:\n[combobox] Reservation type -> \nSELECT: Pickup\n[svg] -> CLICK \n[searchbox] Find a location -> TYPE: \nBostonRanking LMClassifier[0, 1]\nScoreFigure 4: Illustration of the candidate gener-\nation module and the templates for construct-\ning task query and candidate representation.\n<html> <form id=0> <div meta=\"navigation; sitelinks\" >\n<p> <a> Collect Renaissance </a> <a> Shop Le Meridien \n</a> <a> Westin Store </a> <a> Sheraton Store </a>\n</p> </div> \u2026 <div> <select id=1 meta=\"Size; Select a \nSize\"> <span meta=tablist > <button id=2meta=\"button; \ntab\">Description </button> \u2026 <a id=3meta=\"Shop \nFeather & Down Pillow\" > <img meta=\"Product Feather & \nDown Pillow\" > <p> <a> California Privacy Rights </a>\n<a>Privacy Statement </a> <a> Terms of Use </a> <a\nid=4>Loyalty Terms </a> \u2026\nBased on the HTML webpage above, try to complete the following \ntask:\nTask: Search for queen-size pillow protectors from the Marriot \nshop, and if found, add two pieces to the cart and checkout. \nPrevious actions: \n[button] Special Offers -> CLICK \n[link] Shop Marriott Opens a new window -> CLICK\n[menuitem] category pillows -> CLICK \n[span] Pillow Protector -> CLICK \nWhat should be the next action? Prediction LLM\nPlease select from the following choices (If the correct action is not in the page \nabove, please select A. 'None of the above\u2019): \nA. None of the above\nB.<formid=0> <divmeta=\"navigation; sitelinks\" > <p> <a> Collect \nRenaissance </a> <a> Shop Le Meridien </a> <a> Westin Store </a> <a>\nC.<selectid=1  meta=\"Size; Select a Size\" >\nD.<buttonid=2  meta=\"button; tab\" >Description </button>\nE.<aid=3meta=\"Shop Feather & Down Pillow\" > <imgmeta=\"Product Feather \n& Down Pillow\" > <span> Feather & Down Pillow </span> </a>\nF.<aid=4>Loyalty Terms </a>C. \nAction: SELECT \nValue: QueenElement: <selectid=1  meta=\"Size; Select a Size\" >\nAction: SELECT \nValue: Queen\nMultichoiceDirect Generation\nFigure 5: Illustration of action prediction with LLMs.\ncandidate DOM elements from the webpage that best align with both the task description and the\ncurrent step. We formulate the task query by concatenating the task description with previous actions.\nThe textual representation of each candidate DOM element is derived from a combination of the\nelement\u2019s tag, its textual content, and salient attribute values, as well as the textual representation\nof its parent and child elements. As shown in Figure 4, we pair each DOM element with the task\nquery and feed it to an encoder-only LM through the cross-encoder architecture [ 28], yielding a\nmatching score. At training time, we randomly sample negative elements from the webpage, and use\nthe target element as the positive example. The matching score is passed through a sigmoid activation\nfunction and optimized with a binary cross entropy loss. At inference time, we score all elements in\nthe webpage and pick the top- kelements with the largest logits as input to the second stage.\n3.2 Action Prediction with LLMs\nAfter obtaining the top- kcandidates, we utilize the candidate set to prune the webpage snapshot and\nconstruct snippets that only include the selected candidates and their neighbours as inputs to an LLM.\nRecent studies [ 10,13] have suggested that training LMs for discrimination rather than generation is\nmore generalizable and sample-efficient for other grounding tasks. Inspired by that, we convert the\ntask of element selection into a multi-choice question answering (QA) problem. Instead of generating\nthe complete target element, the LM is trained to instead select from a list of options.", "start_char_idx": 20607, "end_char_idx": 24574, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b49ce16f-348c-4cd1-8cb6-9dcc74e7ae4c": {"__data__": {"id_": "b49ce16f-348c-4cd1-8cb6-9dcc74e7ae4c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19d730d0-825a-4439-885a-dbd50d1bc147", "node_type": "1", "metadata": {}, "hash": "6adecc8eb0a7005c9d684de599eca3258a4b9183c5cc081ed6e4480d899b5195", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e544e3f6-6ab6-4f5d-a2b2-b98da9651da5", "node_type": "1", "metadata": {}, "hash": "639dd20eb6b4a5599ae663ab86674e2cf04f95c813929fe746403a8e0e0ebef0", "class_name": "RelatedNodeInfo"}}, "text": "The matching score is passed through a sigmoid activation\nfunction and optimized with a binary cross entropy loss. At inference time, we score all elements in\nthe webpage and pick the top- kelements with the largest logits as input to the second stage.\n3.2 Action Prediction with LLMs\nAfter obtaining the top- kcandidates, we utilize the candidate set to prune the webpage snapshot and\nconstruct snippets that only include the selected candidates and their neighbours as inputs to an LLM.\nRecent studies [ 10,13] have suggested that training LMs for discrimination rather than generation is\nmore generalizable and sample-efficient for other grounding tasks. Inspired by that, we convert the\ntask of element selection into a multi-choice question answering (QA) problem. Instead of generating\nthe complete target element, the LM is trained to instead select from a list of options. For comparison,\nwe also include a baseline that directly generates the target element based on the provided webpage\nsnippet. In both cases, we directly let the LLM generate the operation, along with the additional value\nneeded for some operations. An example is shown in Figure 5. We incorporate up to 5candidate\nelements within each input, together with a None option, and partition the candidate set into several\ngroups. During training, we construct the target sequence using ground-truth actions and fine-tune\nthe model using a left-to-right language modeling objective. During inference, we divide the top- k\ncandidates into multiple clusters of five options. If more than one option is selected after a round, we\nform new groups with the selected ones. This process repeats until a single element is selected, or all\noptions are rejected by the model, i.e., the model chooses the None option for all groups.\n6\n\nTable 2: Main results. The classification baseline uses DeBERTa Band the generation baseline uses\nFlan-T5 B. For step-wise metrics, we report macro average across tasks.\u2217For GPT-4 we use 50tasks\nfor each setting with top- 10candidates due to limited budget. See Appendix D.3 for results on the 50\ntasks subsets for all methods.\nCross-Task Cross-Website Cross-Domain\nEle. Acc Op. F1 Step SR SR Ele. Acc Op. F1 Step SR SR Ele. Acc Op. F1 Step SR SR\nClassification 26.8 \u2212 \u2212 \u2212 21.6 \u2212 \u2212 \u2212 24.5 \u2212 \u2212 \u2212\nGeneration 20.2 52 .0 17 .5 0 .0 13 .9 44 .7 11 .0 0 .0 14 .2 44 .7 11 .9 0 .4\nMINDACT\nw/ Flan-T5 B 43.6 76 .8 41 .0 4 .0 32 .1 67.6 29.5 1 .7 33 .9 67.3 31.6 1 .6\nw/ Flan-T5 L 53.4 75.7 50.3 7.1 39.2 67 .1 35 .3 1 .1 39 .7 67 .2 37 .3 2 .7\nw/ Flan-T5 XL 55.1 75 .7 52 .0 5.2 42.0 65.2 38.9 5 .1 42 .1 66.5 39.6 2 .9\nw/ GPT-3.5 20.3 56 .6 17 .4 0 .8 19 .3 48 .8 16 .2 0 .6 21 .6 52 .8 18 .6 1 .0\nw/ GPT-4\u221741.6 60 .6 36 .2 2 .0 35 .8 51 .1 30 .1 2 .0 37 .1 46 .5 26 .4 2 .0\n4 Experiments\n4.1 Experimental Setup\nThe diversity of MIND2W EBprovides a unique opportunity to evaluate an agent\u2019s generalizability at\ndifferent levels. We seek to understand how well an agent can generalize across domains, websites,\nand tasks: Test Cross-Domain , for which we hold out two top-level domains, Information andService ,\nwith 912tasks from 73websites. Here, the model is expected to generalize to an entirely new\ndomain without having seen any websites or tasks associated with that domain during training.\nTest Cross-Website , with 10websites from each remaining top-level domain, containing 177tasks. In\nthis setting, the model is never exposed to the test websites during training.", "start_char_idx": 23694, "end_char_idx": 27159, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e544e3f6-6ab6-4f5d-a2b2-b98da9651da5": {"__data__": {"id_": "e544e3f6-6ab6-4f5d-a2b2-b98da9651da5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b49ce16f-348c-4cd1-8cb6-9dcc74e7ae4c", "node_type": "1", "metadata": {}, "hash": "9443259cdd3d3922869a2ba3a424ae96c74a150b7e78d9c99c5339838044f48b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9afb300-8619-430f-9ff8-06ed71a402be", "node_type": "1", "metadata": {}, "hash": "5e6ceee2799f967f71fc9218ac0e66a51373c9a64fe68854b991fb08eadf5f98", "class_name": "RelatedNodeInfo"}}, "text": "We seek to understand how well an agent can generalize across domains, websites,\nand tasks: Test Cross-Domain , for which we hold out two top-level domains, Information andService ,\nwith 912tasks from 73websites. Here, the model is expected to generalize to an entirely new\ndomain without having seen any websites or tasks associated with that domain during training.\nTest Cross-Website , with 10websites from each remaining top-level domain, containing 177tasks. In\nthis setting, the model is never exposed to the test websites during training. However, it has been\ntrained on websites from the same domain and possibly with similar tasks. This setup allows us\nto assess an agent\u2019s ability to adapt to entirely new websites, yet within familiar domains and task\ncontexts. Test Cross-Task , where we randomly split 20% of the remaining data, regardless of domains\nand websites, resulting in 252tasks from 69websites. In this setting, the model has been exposed to\nwebpages from the same website during training and has likely encountered similar tasks. The rest of\ndata is used for training, which contains 1,009tasks from 73websites.\n4.2 Data Preprocessing and Evaluation\nWe apply simple heuristics to clean the raw HTML documents, keeping only elements that are\nvisible and carry substantial semantic meaning, as determined by their attributes, textual content, and\nneighboring elements. This effectively reduces the average number of elements from 1,135to580,\nwhile still maintaining an overall recall of 94.7% for the target element in the training data.\nFor evaluation, we first calculate Element Accuracy that compares the selected element with\nall acceptable elements, and Operation F1 that calculates token-level F1 score for the predicted\noperation. This is the same as accuracy for Click , but considers the correctness of the input value for\nType andSelect Option . Each step of the task is evaluated independently with the ground truth\naction history provided. We then define Step Success Rate andSuccess Rate (for the whole task). A\nstep is regarded as successful only if both the selected element and the predicted operation are correct.\nA task is regarded successful only if all steps have succeeded. It is therefore a stringent metric. For\nstep-wise metrics, we report macro average across tasks.\n4.3 Results\nCandidate Generation. We fine-tune DeBERTa [ 16] as the small LM for candidate generation.\nAs candidate generation requires high efficiency, we use the base version DeBERTa Bwith 86M\nparameters. Overall, it achieves 88.9% /85.3% /85.7% Recall@ 50on Test Cross-Task , Test Cross-Website\nand Test Cross-Domain , respectively. We use its top- 50ranking results as the candidate pool for all\nsubsequent experiments.\nAction Prediction. We mainly compare against two baselines in Table 2. The first directly uses\nthe candidate generation model (DeBERTa) for element selection, which is similar to existing\nwork [ 14,35] that combines an encoder with classification heads. However, such a design cannot\n7\n\nFigure 6: Step success rate per website grouped by the three splits: Test Cross-Task , Test Cross-Website and\nTest Cross-Domain from left to right. Here we only show websites with more than three test tasks.\nbenefit from many recent LMs that use an encoder-decoder or decoder-only architecture. It cannot\npredict actions and the element selection performance is also not competitive, as shown in Table 2.\nWe use Flan-T5 [ 10] as the backbone for the generation model. The autoregressive generation\nformulation (Figure 5 top) does not perform well, and even underperforms the classification baseline\non element selection despite the larger model size ( 220M for Flan-T5 B). We observe a substantial\ngain with MINDACTusing the multi-choice QA formulation. The best model achieves 52.0% step\nsuccess rate under Cross-Task setting, and 38.9% /39.6% when generalizing to unseen websites\nand domains. However, the overall task success rate remains low for all models, as the agent often\ncommits at least one error step in most cases.\nThree Levels of Generalization. All models perform best on the Cross-Task setting, with over\n10% absolute gap (step SR) on average compared with Cross-Website and Cross-Domain settings,\nindicating that generalizing to unseen environments is still a major challenge. On the contrary, we\nnote that the performance of Cross-Website and Cross-Domain settings are notably similar, which is\nalso reinforced in Figure 6, where there is no clear distinction in performance across these settings.", "start_char_idx": 26614, "end_char_idx": 31153, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9afb300-8619-430f-9ff8-06ed71a402be": {"__data__": {"id_": "a9afb300-8619-430f-9ff8-06ed71a402be", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e544e3f6-6ab6-4f5d-a2b2-b98da9651da5", "node_type": "1", "metadata": {}, "hash": "639dd20eb6b4a5599ae663ab86674e2cf04f95c813929fe746403a8e0e0ebef0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b261699b-4d9a-4888-888b-696381fca41a", "node_type": "1", "metadata": {}, "hash": "f62a02104b3f9c33e29ef1e1109b505554b629ff75faca2b596126b1e388205f", "class_name": "RelatedNodeInfo"}}, "text": "We observe a substantial\ngain with MINDACTusing the multi-choice QA formulation. The best model achieves 52.0% step\nsuccess rate under Cross-Task setting, and 38.9% /39.6% when generalizing to unseen websites\nand domains. However, the overall task success rate remains low for all models, as the agent often\ncommits at least one error step in most cases.\nThree Levels of Generalization. All models perform best on the Cross-Task setting, with over\n10% absolute gap (step SR) on average compared with Cross-Website and Cross-Domain settings,\nindicating that generalizing to unseen environments is still a major challenge. On the contrary, we\nnote that the performance of Cross-Website and Cross-Domain settings are notably similar, which is\nalso reinforced in Figure 6, where there is no clear distinction in performance across these settings.\nThis suggests that the challenges primarily stem from the diversity in website designs and interaction\nlogic rather than domain specifics. Tasks across domains tend to share common operations, and\npretrained LMs may already have the capability to decompose complex tasks at a high level based\non commonsense knowledge. Yet, grounding such knowledge into actionable steps in specific and\nvarying environments remains a considerable challenge.\nIn-context Learning with LLMs. We also experiment with two popular LLMs, GPT-3.5-turbo and\nGPT-4 [ 25], through in-context learning. We use the same multiple-choice formulation as MINDACT,\nand include three demonstration examples for in-context learning. We can see that both models\nare comparable to the two baselines with only three in-context examples. Note that this is not a\nfair comparison with the Flan-T5 models, which are fine-tuned on the full training data. We also\ninclude the zero-shot results with Flan-T5 XLin Appendix D.2, but the model fails to perform the task\nwithout fine-tuning. Meanwhile, GPT-3.5 only has around 20% element selection accuracy, despite\nthe superior performance people have observed on other datasets. Further analysis reveals that one\npossible problem is the model\u2019s propensity to select the None option, asserting that the task cannot\nbe finished on the current webpage. This is somewhat accurate since tasks typically necessitate\nnavigation through multiple webpages and performing a series of actions before reaching the final\nresult. This aspect indeed represents the primary difficulty of our task. On the other hand, we observe\nhighly promising outcomes with GPT-4. The performance is on par with the tuned Flan-T5 models\nunder Cross-Website and Cross-Domain settings for element selection, indicating a great potential\nfor developing generalist agents using LLMs. Nevertheless, GPT-4\u2019s high operational cost remains a\nconcern. Developing smaller models specialized for the web is an interesting future avenue.\n5 Related Work\nAutonomous Agents for Web and Mobile Applications. Considerable initiatives have been in-\nvested in automating web navigation, driven by a vision of facilitating effortless human-web interac-\ntion. Yet, previous research has been limited by the types of tasks and websites it can handle, either\nconfined to simplified simulation environments [ 22,31,40], or limited to a narrow set of domains and\nwebsites [ 39,40]. Recent studies [ 5,21,35] have utilized similar techniques for mobile applications,\nhowever, these are often simpler and offer fewer functions compared with full-fledged websites. In\ncontrast, MIND2W EBaims to adapt to a realistic web environment, characterized by its high diversity.\n8\n\nAlso related is the research on web automation systems [ 1,19]. These technologies often demand\nprogramming skills, which can make them less accessible to general users. We aim to equip the web\nautomation system with a natural language interface, thereby reducing the entry barrier significantly.\nLarge Language Models. In recent years, there has been a surge in the development and application\nof large language models (LLMs). These models, often encompassing billions of parameters, are\npre-trained on massive corpora of text data [ 3,44,45], enabling them to capture intricate linguistic\npatterns, nuances, and relationships, resulting in unprecedented performance on a wide array of\nNLP tasks. One of the most noteworthy attributes of LLMs is their few-shot learning capability.\nUnlike traditional machine learning models that necessitate extensive labeled data for task-specific\nfine-tuning, LLMs can often perform tasks with minimal task-specific examples.", "start_char_idx": 30311, "end_char_idx": 34833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b261699b-4d9a-4888-888b-696381fca41a": {"__data__": {"id_": "b261699b-4d9a-4888-888b-696381fca41a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9afb300-8619-430f-9ff8-06ed71a402be", "node_type": "1", "metadata": {}, "hash": "5e6ceee2799f967f71fc9218ac0e66a51373c9a64fe68854b991fb08eadf5f98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6580de09-1c7e-4d29-87d2-f4a62f8b9f05", "node_type": "1", "metadata": {}, "hash": "743ac75b22ffb5cbbe385b2356915bcaa15bd786710f4dcc41331afe4edeb170", "class_name": "RelatedNodeInfo"}}, "text": "8\n\nAlso related is the research on web automation systems [ 1,19]. These technologies often demand\nprogramming skills, which can make them less accessible to general users. We aim to equip the web\nautomation system with a natural language interface, thereby reducing the entry barrier significantly.\nLarge Language Models. In recent years, there has been a surge in the development and application\nof large language models (LLMs). These models, often encompassing billions of parameters, are\npre-trained on massive corpora of text data [ 3,44,45], enabling them to capture intricate linguistic\npatterns, nuances, and relationships, resulting in unprecedented performance on a wide array of\nNLP tasks. One of the most noteworthy attributes of LLMs is their few-shot learning capability.\nUnlike traditional machine learning models that necessitate extensive labeled data for task-specific\nfine-tuning, LLMs can often perform tasks with minimal task-specific examples. Furthermore, LLMs\nsuch as GPT-3 [ 4] and PaLM [ 9] have also demonstrated the ability to do in-context learning, where\nthey can adapt to novel tasks by simply providing context within the input prompt, eliminating the\nneed for explicit retraining. In this work, we explore the use of LLMs to build generalist agent on top\nofMIND2W EBby either tuning medium-sized LMs with only around 1,000examples, or prompting\nan LLM such as GPT-4, and have observed promising results.\nGrounded Language Understanding. Our work also aligns with the field of grounded language\nunderstanding, which aims to map natural language utterances onto executable plans in a target\nenvironment [ 13]. Many studies have centered around environments underpinned by a well-structured\nschema or ontology, including relational databases [ 36,43] and knowledge bases [ 12,42], which may\nnot adequately reflect the more heterogeneous conditions in real-world situations. Our work instead\ngrounds natural language in the noisy and schemaless web environment. Our setting is also connected\nto embodied AI, where an agent, guided by language instructions, carries out tasks in a physical\nenvironment [ 2,32,33]. Nonetheless, existing research primarily focuses on a specific setting (e.g.,\nhousehold environments), limiting their diversity. MIND2W EBprovides a unique testbed for studying\na broad range of grounding challenges in real-world environments.\nTool Learning. Recent developments have underscored LLMs\u2019 potential in using a myriad of tools\n(i.e., taking actions) to augment their capacity [ 23,27], including search engine, translator, calculator,\netc. Example works include Toolformer [ 29], ReAct [ 41], and ToolkenGPT [ 15]. The creation of\nrecent benchmarks on tool learning [ 20,26] further highlights the growing interest in evaluating\nLLMs\u2019 proficiency in tool usage. However, existing research primarily concentrates on short-term\ntool invocation, neglecting long-term planning. MIND2W EBcan bridge this lacuna by necessitating\nLLMs to take actions within realistic web-browsing environments that demand prolonged decision-\nmaking sequences. Furthermore, MIND2W EBmay stimulate the development of more advanced\ntools based on LLMs that interface the web with natural language. These advanced tools could be\nsubsequently employed by another LLM for more challenging problem-solving tasks [11, 30].\n6 Limitations and Potential Societal Impact\nMIND2W EBis designed to facilitate the development and evaluation of generalist agents for the web.\nSuch agents hold great potential for making the web more accessible and easy to use, especially for\nindividuals who are less familiar with information technology or have disabilities and may struggle\nto navigate through complex web apps and get overwhelmed by the options available. However, there\nare still potential concerns and limitations regarding the current data collection, system design and\nsafety for deployment in real world.\nDiversity and Representation in Data Collection. Although we strive to choose representative\nwebsites covering diverse domains, the present selection predominantly comprises English-language\nwebsites primarily used in the U.S. Meanwhile, all our annotators are sourced through the Amazon\nMTurk platform, which might be biased towards a group that is more proficient in web use. Therefore,\nthe tasks and websites embodied in our dataset may represent only a subset of all potential tasks\nthat can be performed on the web. Bearing this limitation in mind, the design of MIND2W EBand\nour data collection protocol allow for easy expansion to encompass more tasks and websites.", "start_char_idx": 33868, "end_char_idx": 38466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6580de09-1c7e-4d29-87d2-f4a62f8b9f05": {"__data__": {"id_": "6580de09-1c7e-4d29-87d2-f4a62f8b9f05", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b261699b-4d9a-4888-888b-696381fca41a", "node_type": "1", "metadata": {}, "hash": "f62a02104b3f9c33e29ef1e1109b505554b629ff75faca2b596126b1e388205f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "844cd9dc-85f5-4e67-8617-80481d075dc9", "node_type": "1", "metadata": {}, "hash": "7441bb163f7b786aa900d14b2e037b582348607c93b546f85d09bf9a358781f0", "class_name": "RelatedNodeInfo"}}, "text": "However, there\nare still potential concerns and limitations regarding the current data collection, system design and\nsafety for deployment in real world.\nDiversity and Representation in Data Collection. Although we strive to choose representative\nwebsites covering diverse domains, the present selection predominantly comprises English-language\nwebsites primarily used in the U.S. Meanwhile, all our annotators are sourced through the Amazon\nMTurk platform, which might be biased towards a group that is more proficient in web use. Therefore,\nthe tasks and websites embodied in our dataset may represent only a subset of all potential tasks\nthat can be performed on the web. Bearing this limitation in mind, the design of MIND2W EBand\nour data collection protocol allow for easy expansion to encompass more tasks and websites. The\ninclusion of additional websites, potentially from different countries and languages, and tasks from\nmore diverse demographics, such as individuals from different age groups, those traditionally facing\nweb accessibility challenges, and professionals from specific domains like software development,\nresearch, law, and more, present exciting directions for future development.\n9\n\nUse of Multimodal Information. Our current approach, MINDACT, models the web environment\nusing only textual context from webpage snapshots. Nevertheless, crucial information can also be\ngleaned from the visual representation of a rendered webpage. While not currently utilized, we have\nincluded complete webpage snapshots in MIND2W EB, enabling rendering of the webpage for visual\ninterpretation. The use of this multimodal information will be a viable prospect for improving model\nperformance.\nModeling of Interaction Dynamics. InMINDACT, we encode each webpage independently at every\nstep, with only the previous actions provided as historical context. However, the changes of the\nweb environment could also provide significant cues for task completion, such as the appearance\nof a dropdown menu following a button click. Exploring effective ways to model such dynamic\nenvironment transformations during interaction could be an essential aspect for developing robust\nweb agents.\nHuman-Agent Interaction. In the current design of MIND2W EB, the user provides a single\ndescription of the task goal up front, and the agent carries out the task from start to finish. In\nreal-world settings, the user may wish to adjust or add task requirements in the middle, or the agent\nmight seek user confirmation for more accurate task understanding. Extending Mind2Web to an\ninteractive or conversational setting, thereby allowing diverse forms of human-agent interactions,\ncould be an interesting future direction.\nEvaluation with Offline/Online Environments. Following recent works [ 5,35], we evaluate the\nsystem with cached offline environments, which allows us to test using snapshots of complex real-\nworld websites. However, a downside to this is that the task will fail immediately if an action was not\ncached during data collection, potentially leading to false negatives due to the existence of multiple\npaths for completing the same task. As described in Appendix C.1, we normalize the actions to\naddress equivalent elements within the same page. In addition, we include complete network traffic in\nthe dataset, presenting possibilities for future research to enable some degree of replay and exploration\nwithin the cached environment. Given that MIND2W EBfaithfully replicates real-world webpages,\nsystems trained on the dataset should be readily transferable to live websites. Conducting end-to-end\nlive evaluation on real websites with human assistance is a very promising direction that is worth\nexploration.\nSafety in Deployment. While the development of general-purpose web agents holds great potential\nto enhance efficiency, optimize user experiences, and promote web accessibility universally, the\naccompanying safety considerations for real-world deployment cannot be ignored. These include\nhow to effectively manage sensitive actions like financial transactions, enhancing transparency and\ninterpretability, and keeping users in control during task execution. Additionally, there is the risk\nof these agents possessing the capability to breach existing security measures such as CAPTCHA\nand being exploited for malicious activities, such as disseminating false information. Therefore, it is\nalso important for cybersecurity research to consider these potential uses and develop preemptive\nprotective measures.\n7 Conclusion\nIn this work, we introduced MIND2W EB, the first dataset for developing and evaluating generalist\nagents for the web. We also proposed MINDACT, an agent that leverages the power of (large) language\nmodels for effectively tackling this task. Our work opens up a wide range of promising future\ndirections, including integrating multi-modal information, reinforcement learning with feedback\nfrom real websites, and specialized LMs for web understanding and action taking. We hope that\nMIND2W EBwill serve as a valuable platform for the research community to advance towards\ngeneralist agents for the web.", "start_char_idx": 37640, "end_char_idx": 42782, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "844cd9dc-85f5-4e67-8617-80481d075dc9": {"__data__": {"id_": "844cd9dc-85f5-4e67-8617-80481d075dc9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6580de09-1c7e-4d29-87d2-f4a62f8b9f05", "node_type": "1", "metadata": {}, "hash": "743ac75b22ffb5cbbe385b2356915bcaa15bd786710f4dcc41331afe4edeb170", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a5dea97-9e72-4d47-851d-51d7d912346a", "node_type": "1", "metadata": {}, "hash": "65f1cc628518348450810ad0642b80457082c738f5522870d65873db7239f177", "class_name": "RelatedNodeInfo"}}, "text": "Additionally, there is the risk\nof these agents possessing the capability to breach existing security measures such as CAPTCHA\nand being exploited for malicious activities, such as disseminating false information. Therefore, it is\nalso important for cybersecurity research to consider these potential uses and develop preemptive\nprotective measures.\n7 Conclusion\nIn this work, we introduced MIND2W EB, the first dataset for developing and evaluating generalist\nagents for the web. We also proposed MINDACT, an agent that leverages the power of (large) language\nmodels for effectively tackling this task. Our work opens up a wide range of promising future\ndirections, including integrating multi-modal information, reinforcement learning with feedback\nfrom real websites, and specialized LMs for web understanding and action taking. We hope that\nMIND2W EBwill serve as a valuable platform for the research community to advance towards\ngeneralist agents for the web.\nAcknowledgements\nThe authors would thank colleagues from the OSU NLP group for constructive feedback and all\ncontributors from the Amazon Mechanical Turk platform who participated in our study and assisted\nin data collection. This research was sponsored in part by NSF OAC 2112606, NSF CAREER\n#1942980, ARL W911NF2220144 and Ohio Supercomputer Center [ 6]. The views and conclusions\ncontained herein are those of the authors and should not be interpreted as representing the official\n10\n\npolicies, either expressed or implied, of the U.S. government. The U.S. Government is authorized\nto reproduce and distribute reprints for Government purposes notwithstanding any copyright notice\nherein.\nReferences\n[1]Puppeteer headless chrome node.js api. https://github.com/puppeteer/puppeteer ,\n2021.\n[2]Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\nChelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Daniel Ho,\nJasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle\nJeffrey, Sally Jesmonth, Nikhil J. Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang,\nKuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell\nQuiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers,\nClayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nand Mengyuan Yan. Do as I can, not as I say: Grounding language in robotic affordances.\nCoRR , abs/2204.01691, 2022. doi: 10.48550/arXiv.2204.01691. URL https://doi.org/10.\n48550/arXiv.2204.01691 .\n[3]Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman, Simran Arora, Sydney von\nArx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Bryn-\njolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen,\nKathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya,\nEsin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn,\nTrevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha,\nTatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu,\nJing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti,\nGeoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna,\nRohith Kuditipudi, and et al. On the opportunities and risks of foundation models. CoRR ,\nabs/2108.07258, 2021. URL https://arxiv.org/abs/2108.07258 .", "start_char_idx": 41818, "end_char_idx": 45350, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a5dea97-9e72-4d47-851d-51d7d912346a": {"__data__": {"id_": "5a5dea97-9e72-4d47-851d-51d7d912346a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "844cd9dc-85f5-4e67-8617-80481d075dc9", "node_type": "1", "metadata": {}, "hash": "7441bb163f7b786aa900d14b2e037b582348607c93b546f85d09bf9a358781f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be73ca2b-08ae-4ae5-a08b-5f503f529c49", "node_type": "1", "metadata": {}, "hash": "37fe86e259f24f913b633e18706fd4b65106185aa5a27e177488febd77b588c6", "class_name": "RelatedNodeInfo"}}, "text": "On the opportunities and risks of foundation models. CoRR ,\nabs/2108.07258, 2021. URL https://arxiv.org/abs/2108.07258 .\n[4]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle,\nM. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information\nProcessing Systems , volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020.\n[5]Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate Saenko, and Bryan A. Plum-\nmer. A dataset for interactive vision-language navigation with unknown command feasibility.\nInEuropean Conference on Computer Vision , 2022.\n[6]Ohio Supercomputer Center. Ohio supercomputer center, 1987. URL http://osc.edu/ark:\n/19495/f5s1ph73 .\n[7]Radoslav Chakarov. How many websites are there? how many are active in 2023? https:\n//webtribunal.net/blog/how-many-websites/ . 2023.\n[8]Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to answer\nopen-domain questions. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) , pages 1870\u20131879, Vancouver, Canada,\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL\nhttps://aclanthology.org/P17-1171 .\n[9]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay,\nNoam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope,\n11\n\nJames Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke,\nAnselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant\nMisra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek\nLim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal,\nMark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor\nLewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou,\nXuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language\nmodeling with pathways, 2023. URL http://jmlr.org/papers/v24/22-1144.html .\n[10] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav\nMishra, Adams Yu, Vincent Y .", "start_char_idx": 45230, "end_char_idx": 48328, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be73ca2b-08ae-4ae5-a08b-5f503f529c49": {"__data__": {"id_": "be73ca2b-08ae-4ae5-a08b-5f503f529c49", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a5dea97-9e72-4d47-851d-51d7d912346a", "node_type": "1", "metadata": {}, "hash": "65f1cc628518348450810ad0642b80457082c738f5522870d65873db7239f177", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f93a121e-470a-40bb-abb9-967d52be7211", "node_type": "1", "metadata": {}, "hash": "d2d747a5f5888af7ea98dcfd25487b4799580c185b58f71471c0b21303629311", "class_name": "RelatedNodeInfo"}}, "text": "Palm: Scaling language\nmodeling with pathways, 2023. URL http://jmlr.org/papers/v24/22-1144.html .\n[10] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav\nMishra, Adams Yu, Vincent Y . Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le, and\nJason Wei. Scaling instruction-finetuned language models. CoRR , abs/2210.11416, 2022. doi:\n10.48550/arXiv.2210.11416. URL https://doi.org/10.48550/arXiv.2210.11416 .\n[11] Yingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan, Shuyuan Xu, and Yongfeng Zhang.\nOpenagi: When LLM meets domain experts. CoRR , abs/2304.04370, 2023. doi: 10.48550/\narXiv.2304.04370. URL https://doi.org/10.48550/arXiv.2304.04370 .\n[12] Yu Gu, Sue Kase, Michelle Vanni, Brian M. Sadler, Percy Liang, Xifeng Yan, and Yu Su.\nBeyond I.I.D.: three levels of generalization for question answering on knowledge bases. In\nJure Leskovec, Marko Grobelnik, Marc Najork, Jie Tang, and Leila Zia, editors, WWW \u201921: The\nWeb Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021 , pages 3477\u20133488.\nACM / IW3C2, 2021. doi: 10.1145/3442381.3449992. URL https://doi.org/10.1145/\n3442381.3449992 .\n[13] Yu Gu, Xiang Deng, and Yu Su. Don\u2019t generate, discriminate: A proposal for grounding\nlanguage models to real-world environments. CoRR , abs/2212.09736, 2022. doi: 10.48550/\narXiv.2212.09736. URL https://doi.org/10.48550/arXiv.2212.09736 .\n[14] Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowd-\nhery, Sharan Narang, Noah Fiedel, and Aleksandra Faust. Understanding html with large\nlanguage models, 2023.\n[15] Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. Toolkengpt: Augmenting frozen\nlanguage models with massive tools via tool embeddings. CoRR , abs/2305.11554, 2023. doi:\n10.48550/arXiv.2305.11554. URL https://doi.org/10.48550/arXiv.2305.11554 .\n[16] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: decoding-enhanced\nbert with disentangled attention. In 9th International Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. URL https:\n//openreview.net/forum?id=XPZIaotutsD .\n[17] Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-Rong Wen. Struct-\ngpt: A general framework for large language model to reason over structured data. CoRR ,\nabs/2305.09645, 2023. doi: 10.48550/arXiv.2305.09645. URL https://doi.org/10.48550/\narXiv.2305.09645 .\n[18] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov,\nDanqi Chen, and Wen-tau Yih.", "start_char_idx": 47943, "end_char_idx": 50752, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f93a121e-470a-40bb-abb9-967d52be7211": {"__data__": {"id_": "f93a121e-470a-40bb-abb9-967d52be7211", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be73ca2b-08ae-4ae5-a08b-5f503f529c49", "node_type": "1", "metadata": {}, "hash": "37fe86e259f24f913b633e18706fd4b65106185aa5a27e177488febd77b588c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0aee29e8-1b66-41f9-ac12-9d47a48e9455", "node_type": "1", "metadata": {}, "hash": "e8817c972383b47dce3473bc142b0e794d24fd51d7122032d72396a04f35f437", "class_name": "RelatedNodeInfo"}}, "text": "OpenReview.net, 2021. URL https:\n//openreview.net/forum?id=XPZIaotutsD .\n[17] Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-Rong Wen. Struct-\ngpt: A general framework for large language model to reason over structured data. CoRR ,\nabs/2305.09645, 2023. doi: 10.48550/arXiv.2305.09645. URL https://doi.org/10.48550/\narXiv.2305.09645 .\n[18] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov,\nDanqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering.\nInProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP) , pages 6769\u20136781, Online, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.\nemnlp-main.550 .\n[19] Gilly Leshed, Eben M. Haber, Tara Matthews, and Tessa Lau. CoScripter: Automating &\nsharing how-to knowledge in the enterprise. In Proceedings of the SIGCHI Conference on\nHuman Factors in Computing Systems , pages 1719\u20131728, Florence Italy, April 2008. ACM.\nISBN 978-1-60558-011-1. doi: 10.1145/1357054.1357323.\n12\n\n[20] Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin\nLi. Api-bank: A benchmark for tool-augmented llms. CoRR , abs/2304.08244, 2023. doi:\n10.48550/arXiv.2304.08244. URL https://doi.org/10.48550/arXiv.2304.08244 .\n[21] Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language\ninstructions to mobile UI action sequences. In Dan Jurafsky, Joyce Chai, Natalie Schluter,\nand Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July 5-10, 2020 , pages 8198\u20138210. Association\nfor Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.729. URL https:\n//doi.org/10.18653/v1/2020.acl-main.729 .\n[22] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement\nlearning on web interfaces using workflow-guided exploration. In 6th International Conference\non Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018,\nConference Track Proceedings . OpenReview.net, 2018. URL https://openreview.net/\nforum?id=ryTp3f-0- .\n[23] Gr\u00e9goire Mialon, Roberto Dess\u00ec, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru,\nRoberta Raileanu, Baptiste Rozi\u00e8re, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard\nGrave, Yann LeCun, and Thomas Scialom. Augmented language models: a survey. CoRR ,\nabs/2302.07842, 2023. doi: 10.48550/arXiv.2302.07842. URL https://doi.org/10.48550/\narXiv.2302.07842 .\n[24] OpenAI. Chatgpt plugins. https://openai.com/blog/chatgpt-plugins . 2023.\n[25] OpenAI. Gpt-4 technical report, 2023.\n[26] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language\nmodel connected with massive apis. CoRR , abs/2305.15334, 2023. doi: 10.48550/arXiv.2305.\n15334.", "start_char_idx": 50276, "end_char_idx": 53218, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0aee29e8-1b66-41f9-ac12-9d47a48e9455": {"__data__": {"id_": "0aee29e8-1b66-41f9-ac12-9d47a48e9455", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f93a121e-470a-40bb-abb9-967d52be7211", "node_type": "1", "metadata": {}, "hash": "d2d747a5f5888af7ea98dcfd25487b4799580c185b58f71471c0b21303629311", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31ec87f2-0d33-4194-922f-c094e260481a", "node_type": "1", "metadata": {}, "hash": "e66f42a17a1012541baed21e935b187e9c1041ca442616f6970048831530355e", "class_name": "RelatedNodeInfo"}}, "text": "Augmented language models: a survey. CoRR ,\nabs/2302.07842, 2023. doi: 10.48550/arXiv.2302.07842. URL https://doi.org/10.48550/\narXiv.2302.07842 .\n[24] OpenAI. Chatgpt plugins. https://openai.com/blog/chatgpt-plugins . 2023.\n[25] OpenAI. Gpt-4 technical report, 2023.\n[26] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language\nmodel connected with massive apis. CoRR , abs/2305.15334, 2023. doi: 10.48550/arXiv.2305.\n15334. URL https://doi.org/10.48550/arXiv.2305.15334 .\n[27] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng,\nYufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng\nQian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining\nYe, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi\nLu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang,\nCheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with\nfoundation models. CoRR , abs/2304.08354, 2023. doi: 10.48550/arXiv.2304.08354. URL\nhttps://doi.org/10.48550/arXiv.2304.08354 .\n[28] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing . Association for Computational Linguistics, 11 2019. URL https://arxiv.org/\nabs/1908.10084 .\n[29] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettle-\nmoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\nthemselves to use tools. CoRR , abs/2302.04761, 2023. doi: 10.48550/arXiv.2302.04761. URL\nhttps://doi.org/10.48550/arXiv.2302.04761 .\n[30] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hug-\nginggpt: Solving AI tasks with chatgpt and its friends in huggingface. CoRR , abs/2303.17580,\n2023. doi: 10.48550/arXiv.2303.17580. URL https://doi.org/10.48550/arXiv.2303.\n17580 .\n[31] Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of Bits:\nAn Open-Domain Platform for Web-Based Agents. In Proceedings of the 34th International\nConference on Machine Learning , pages 3135\u20133144. PMLR, July 2017.\n[32] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh\nMottaghi, Luke Zettlemoyer, and Dieter Fox. ALFRED: A benchmark for interpreting grounded\ninstructions for everyday tasks. In 2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020 , pages 10737\u201310746.\n13\n\nComputer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600.2020.01075.", "start_char_idx": 52758, "end_char_idx": 55457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31ec87f2-0d33-4194-922f-c094e260481a": {"__data__": {"id_": "31ec87f2-0d33-4194-922f-c094e260481a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0aee29e8-1b66-41f9-ac12-9d47a48e9455", "node_type": "1", "metadata": {}, "hash": "e8817c972383b47dce3473bc142b0e794d24fd51d7122032d72396a04f35f437", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fd5bcc5-308b-4b1e-94f7-a3c19d73d366", "node_type": "1", "metadata": {}, "hash": "701303db6caad0b6077fc31c31dbfddd6ad63a87c5c6e797169d0ad583235956", "class_name": "RelatedNodeInfo"}}, "text": "World of Bits:\nAn Open-Domain Platform for Web-Based Agents. In Proceedings of the 34th International\nConference on Machine Learning , pages 3135\u20133144. PMLR, July 2017.\n[32] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh\nMottaghi, Luke Zettlemoyer, and Dieter Fox. ALFRED: A benchmark for interpreting grounded\ninstructions for everyday tasks. In 2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020 , pages 10737\u201310746.\n13\n\nComputer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600.2020.01075. URL\nhttps://openaccess.thecvf.com/content_CVPR_2020/html/Shridhar_ALFRED_\nA_Benchmark_for_Interpreting_Grounded_Instructions_for_Everyday_Tasks_\nCVPR_2020_paper.html .\n[33] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su.\nLlm-planner: Few-shot grounded planning for embodied agents with large language models.\nCoRR , abs/2212.04088, 2022. doi: 10.48550/arXiv.2212.04088. URL https://doi.org/10.\n48550/arXiv.2212.04088 .\n[34] Yu Su, Ahmed Hassan Awadallah, Madian Khabsa, Patrick Pantel, Michael Gamon, and Mark J.\nEncarnaci\u00f3n. Building natural language interfaces to web apis. In Ee-Peng Lim, Marianne\nWinslett, Mark Sanderson, Ada Wai-Chee Fu, Jimeng Sun, J. Shane Culpepper, Eric Lo, Joyce C.\nHo, Debora Donato, Rakesh Agrawal, Yu Zheng, Carlos Castillo, Aixin Sun, Vincent S. Tseng,\nand Chenliang Li, editors, Proceedings of the 2017 ACM on Conference on Information and\nKnowledge Management, CIKM 2017, Singapore, November 06 - 10, 2017 , pages 177\u2013186.\nACM, 2017. doi: 10.1145/3132847.3133009. URL https://doi.org/10.1145/3132847.\n3133009 .\n[35] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. META-GUI:\nTowards Multi-modal Conversational Agents on Mobile GUI, November 2022.\n[36] Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. RAT-\nSQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers. In Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics , pages 7567\u20137578,\nOnline, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.677.\n[37] Kyle Williams, Seyyed Hadi Hashemi, and Imed Zitouni. Automatic task completion flows\nfrom web apis. In Benjamin Piwowarski, Max Chevalier, \u00c9ric Gaussier, Yoelle Maarek,\nJian-Yun Nie, and Falk Scholer, editors, Proceedings of the 42nd International ACM SIGIR\nConference on Research and Development in Information Retrieval, SIGIR 2019, Paris, France,\nJuly 21-25, 2019 , pages 1009\u20131012. ACM, 2019. doi: 10.1145/3331184.3331318. URL\nhttps://doi.org/10.1145/3331184.3331318 .\n[38] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-\nart natural language processing.", "start_char_idx": 54861, "end_char_idx": 57943, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9fd5bcc5-308b-4b1e-94f7-a3c19d73d366": {"__data__": {"id_": "9fd5bcc5-308b-4b1e-94f7-a3c19d73d366", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31ec87f2-0d33-4194-922f-c094e260481a", "node_type": "1", "metadata": {}, "hash": "e66f42a17a1012541baed21e935b187e9c1041ca442616f6970048831530355e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12955511-a656-4d76-b211-062c0c679438", "node_type": "1", "metadata": {}, "hash": "a698dbe135278d73bfd44e1497cfe99478c68168fd30af752cfc6de21116ea6b", "class_name": "RelatedNodeInfo"}}, "text": "ACM, 2019. doi: 10.1145/3331184.3331318. URL\nhttps://doi.org/10.1145/3331184.3331318 .\n[38] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-\nart natural language processing. In Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: System Demonstrations , pages 38\u201345, Online, October 2020.\nAssociation for Computational Linguistics. URL https://www.aclweb.org/anthology/\n2020.emnlp-demos.6 .\n[39] Nancy Xu, Sam Masling, Michael Du, Giovanni Campagna, Larry Heck, James A. Landay, and\nMonica S. Lam. Grounding open-domain instructions to automate web support tasks. In North\nAmerican Chapter of the Association for Computational Linguistics , 2021.\n[40] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. WebShop: Towards Scalable\nReal-World Web Interaction with Grounded Language Agents. July 2022. doi: 10.48550/arXiv.\n2207.01206.\n[41] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. CoRR , abs/2210.03629, 2022.\ndoi: 10.48550/arXiv.2210.03629. URL https://doi.org/10.48550/arXiv.2210.03629 .\n[42] Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. Semantic Parsing via Staged\nQuery Graph Generation: Question Answering with Knowledge Base. In Proceedings of the\n53rd Annual Meeting of the Association for Computational Linguistics and the 7th International\nJoint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 1321\u20131331,\nBeijing, China, 2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-1128.\n14\n\n[43] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene\nLi, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir R. Radev. Spider: A large-scale\nhuman-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In\nEllen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii, editors, Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium,\nOctober 31 - November 4, 2018 , pages 3911\u20133921. Association for Computational Linguistics,\n2018. doi: 10.18653/v1/d18-1425. URL https://doi.org/10.18653/v1/d18-1425 .\n[44] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian\nMin, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng\nChen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie,\nand Ji-Rong Wen. A survey of large language models. CoRR , abs/2303.18223, 2023. doi:\n10.48550/ARXIV .2303.18223. URL https://doi.org/10.48550/arXiv.2303.18223 .", "start_char_idx": 57464, "end_char_idx": 60431, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12955511-a656-4d76-b211-062c0c679438": {"__data__": {"id_": "12955511-a656-4d76-b211-062c0c679438", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fd5bcc5-308b-4b1e-94f7-a3c19d73d366", "node_type": "1", "metadata": {}, "hash": "701303db6caad0b6077fc31c31dbfddd6ad63a87c5c6e797169d0ad583235956", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3f4f903-820e-4236-8012-b6418af40470", "node_type": "1", "metadata": {}, "hash": "82322e6bc79b1f68c88bb598464f5c324f37b4cc07866c2a4763b38a8cd5b6b4", "class_name": "RelatedNodeInfo"}}, "text": "URL https://doi.org/10.18653/v1/d18-1425 .\n[44] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian\nMin, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng\nChen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie,\nand Ji-Rong Wen. A survey of large language models. CoRR , abs/2303.18223, 2023. doi:\n10.48550/ARXIV .2303.18223. URL https://doi.org/10.48550/arXiv.2303.18223 .\n[45] Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben\nYan, Lifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu, Pengtao Xie, Caiming Xiong, Jian\nPei, Philip S. Yu, and Lichao Sun. A comprehensive survey on pretrained foundation models: A\nhistory from BERT to chatgpt. CoRR , abs/2302.09419, 2023. doi: 10.48550/ARXIV .2302.09419.\nURL https://doi.org/10.48550/arXiv.2302.09419 .\n15\n\nA Overview\nOur supplementary includes the following sections:\n\u2022Section B: Data Collection Details. Details for crowsourcing and implementation details\nfor the three data collection phases: task proposal, task demonstration, and task verification.\n\u2022Section C: Experiment Details. Details for evaluation and model implementation.\n\u2022Section D: Additional Results. Results for additional auxiliary experiments.\nFollowing NeurIPS Dataset and Benchmark track guidelines, we have shared the following artifacts:\nArtifcat Link License\nHomepage https://osu-nlp-group.github.io/Mind2Web/ -\nCode Repository https://github.com/OSU-NLP-Group/Mind2Web MIT License\nTraining Data https://huggingface.co/datasets/osunlp/Mind2Web CC BY 4.0\nTest Data https://shorturl.at/iGI45 (password \u201cmind2web\u201d) CC BY 4.0\nThe authors and the OSU NLP group are committed to ensuring its regular upkeep and updates.\n16\n\nTable 3: Prompt for ChatGPT to generate seed tasks to inspire the annotators during task proposal.\nPrompt We are testing the website\nhttps://aa.com | American Airlines - Airline tickets and low fares at\naa.com\nBook low fares to destinations around the world and find the latest deals on\nairline tickets, hotels, car rentals and vacations at aa.com. As an AAdantage\nmember you earn miles on every trip and everyday spend.\nWe want to test practical daily tasks that a user would do on the web-\nsite.\nCome up with a list of 5 example tasks and try to cover different cases.\nRequirements:\n- Each example should be a single sentence and not just click one of the\nelements.\n- Don\u2019t give step-by-step instructions or directly mention the element to interact.\n- Describe the goal of the task and provide concrete information or constraints.\nUse mock-up information (identifier, number, personal information, name, date,\nattributes, constraints, etc.) to make the task more specific and realistic.\nSample Responses Retrieve the confirmation number for a reservation made under the name jane\nsmith departing from dallas (dfw) to miami (mia) on january 20th.\nFind and book a round-trip flight from new york to london departing on december\n15th and returning on december 22nd for under $800.\nFind the lowest fare for a one-way flight from new york city (jfk) to los angeles\n(lax) leaving on august 15th and returning on august 20th, with no layovers\nexceeding 2 hours.\nRent a car in honolulu for one week starting on august 1st that can fit at least\nfour passengers and has gps navigation included.\nCancel a car rental reservation for a compact car in orlando, florida that was\nscheduled to be picked up on may 25th at 5 pm without incurring any cancellation\nfees.\nB Data Collection Details\nB.1 Crowdsourcing Details\nIn this study, we employ annotators from the Amazon Mechanical Turk (mturk) platform. The\nannotators are required to have a minimum of 1,000approved HITs with an approval rate exceeding\n98% on the platform.", "start_char_idx": 59958, "end_char_idx": 63749, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3f4f903-820e-4236-8012-b6418af40470": {"__data__": {"id_": "e3f4f903-820e-4236-8012-b6418af40470", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12955511-a656-4d76-b211-062c0c679438", "node_type": "1", "metadata": {}, "hash": "a698dbe135278d73bfd44e1497cfe99478c68168fd30af752cfc6de21116ea6b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e37ca8f1-f3e3-4f94-ba1c-ca5f8ce6dc59", "node_type": "1", "metadata": {}, "hash": "b177afa73cc06d3e0fbcefee96a21349569218daca2e45fb709560b6aa731034", "class_name": "RelatedNodeInfo"}}, "text": "Find the lowest fare for a one-way flight from new york city (jfk) to los angeles\n(lax) leaving on august 15th and returning on august 20th, with no layovers\nexceeding 2 hours.\nRent a car in honolulu for one week starting on august 1st that can fit at least\nfour passengers and has gps navigation included.\nCancel a car rental reservation for a compact car in orlando, florida that was\nscheduled to be picked up on may 25th at 5 pm without incurring any cancellation\nfees.\nB Data Collection Details\nB.1 Crowdsourcing Details\nIn this study, we employ annotators from the Amazon Mechanical Turk (mturk) platform. The\nannotators are required to have a minimum of 1,000approved HITs with an approval rate exceeding\n98% on the platform. We design the compensation with an estimated hourly rate of $10.10 to\nrespect the minimum wage guidelines in Ohio, United States. Every worker passing our qualification\nreceives a bonus, and we pay $ 0.80for each approved final task. We do not collect any identifiable\nprivate information during the study, and explicitly instruct the annotators to refrain from entering\npersonal or sensitive data into the system. Annotators engage with our annotation tool only within a\nsecure, remote sandbox environment, posing no foreseeable harm. The study complies with the IRB\nexemption criteria, per the Office of Responsible Research Practices at The Ohio State University.\nAll annotators are presented with a consent form, to which they must agree before participating in\nthe study. To prepare the workers for the task, we provide a comprehensive training document and a\nvideo tutorial, followed by a qualification assessment comprising a questionnaire and a series of test\ndemonstrations using our tool. It is noteworthy that the task is divided into two phases: task proposal\nand task demonstration. The proposal phase comes with a nominal reward, with the majority of the\ncompensation dispensed upon successful completion of the demonstration.\nQuality and diversity are ensured through a two-stage review process. The first author reviews all\ntasks after task proposal and manually select the tasks for demonstration. After task demonstration, a\nthorough final verification of all collected data is conducted by all authors to authenticate the tasks\nand recorded actions. Each demonstration is first verified by one of the authors, and uncertain ones\nare further verified by the first author to reach consensus.\n17\n\nFigure 7: Illustration of our annotation tool, which consists of two side-by-side windows. On the left\nwe provide a dialogue window for the user to control the tool and select operations to take. On the\nright we provide the browser window for the user to interact with and select web elements.\nSelect Website \nand TaskExplore and \nPrepare the \nWebsiteTask \nDemonstrationSelect the \nOperationConfirmthe\nTask\nSelect an elementCompleteDemonstration\nFigure 8: The overall procedure for task demonstration.\nB.2 Task Proposal\nWe use ChatGPT to generate sample tasks to provide inspiration to the annotators, and the prompt\nused is shown in Table 3. For each HIT, we ask the annotator to select a website of their interest\nfirst. Following this, we present them with ten sample tasks produced by ChatGPT, and request\nthem to propose a maximum of five additional tasks. The annotator is instructed not to directly copy\nthe sample tasks. We manually evaluate all submitted tasks and reject those that demonstrate low\nquality or are too similar to previously accepted tasks. We set a nominal reward of $ 0.05for each\ntask proposal HIT, and the annotator will receive it no matter whether the tasks are accepted or not.\nFor accepted ones, the annotator will receive the full reward of $ 0.80on successful demonstration of\nthe task. Once we have collected a total of around 20tasks for a specific website, we desist from\nshowing it to the user, aiming for a balanced distribution among websites and increased task diversity.\nB.3 Task Demonstration\nWe develop a dedicated annotation tool for task demonstration using Playwright,3which allows us to\ninteract with the browser and record user actions. As shown in Figure 7. the tool is composed of two\nwindows. The dialogue window on the left serves as the annotator\u2019s control panel for guiding the\ninteraction flow and choosing operations. The browser window on the right is where the annotator\nnavigates the website and selects elements for interaction. Figure 8 shows the overall procedure\nfor task demonstration. The annotator starts by selecting the website and task to be demonstrated.\nOnce selected, the tool will bring up the website in the browser. The annotator is then instructed\nto explore the website and practice the task.", "start_char_idx": 63018, "end_char_idx": 67733, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e37ca8f1-f3e3-4f94-ba1c-ca5f8ce6dc59": {"__data__": {"id_": "e37ca8f1-f3e3-4f94-ba1c-ca5f8ce6dc59", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3f4f903-820e-4236-8012-b6418af40470", "node_type": "1", "metadata": {}, "hash": "82322e6bc79b1f68c88bb598464f5c324f37b4cc07866c2a4763b38a8cd5b6b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5203e67d-96a2-45d0-ad6f-09d31ee2e4ab", "node_type": "1", "metadata": {}, "hash": "0c0d5ea9730b3fc4c20acc78dfd9e5ceeb137a763ff329b1c0c702c6cb894d94", "class_name": "RelatedNodeInfo"}}, "text": "B.3 Task Demonstration\nWe develop a dedicated annotation tool for task demonstration using Playwright,3which allows us to\ninteract with the browser and record user actions. As shown in Figure 7. the tool is composed of two\nwindows. The dialogue window on the left serves as the annotator\u2019s control panel for guiding the\ninteraction flow and choosing operations. The browser window on the right is where the annotator\nnavigates the website and selects elements for interaction. Figure 8 shows the overall procedure\nfor task demonstration. The annotator starts by selecting the website and task to be demonstrated.\nOnce selected, the tool will bring up the website in the browser. The annotator is then instructed\nto explore the website and practice the task. To collect clean actions during actual demonstration,\nthe workers are asked to close pop-up windows during exploration. We also provide anonymous\naccounts for the workers to use so that no private information is entered. The exploration stage is\n3https://playwright.dev/\n18\n\nFigure 9: Dialogue window for the Type operation.\n Figure 10: Dialogue window for the\nSelect Option operation.\nnot recorded and primarily serves to familiarize the annotator with the website and task, as well as\nto prepare the website to prevent future pop-ups, thereby ensuring a clean, streamlined set of final\nrecorded actions. After exploration, the annotator is directed to return to the homepage and reset any\naltered values, allowing us to begin the demonstration in a fresh state. During the demonstration,\nthe annotator will illustrate how to accomplish the task step-by-step using both the browser and the\ndialogue window. To ensure a clean set of annotated actions, annotators are restricted from directly\nengaging with the browser during the demonstration phase. Instead, we divide each action step into\ntwo stages: Element selection and operation selection. At each step, the annotator first selects the\ntarget element by clicking it in the browser. We will highlight the selected element in the browser\nwindow but block the actual click event. The annotator is then prompted to select the operation to\nperform within the dialogue window, which is then carried out by the annotation tool in the browser.\nWe provide 6operations: Click ,Type ,Hover ,Press Enter ,Click (Fake) andIgnore . For the\nType operation, the annotator is additionally required to supply the value as shown in Figure 9. If\nthe chosen element is a select HTML element, and the annotator opts for Click , it translates to a\nSelect Option operation and we will prompt the annotator to select one of the options as shown\nin Figure 10. To avoid ambiguity, the Click ,Hover andPress Enter operations are all mapped\ntoClick in the final dataset. Click (Fake) is a special operation. It will be recorded the same as\na normal Click but will not get executed in the browser. This is designed for safeguarding against\nstate-changing actions (i.e., actions that produce side effects to the world), such as posting a comment\nor scheduling an appointment, since it will interfere with other real users of the website. In practice,\nonce a model predicts Click (Fake) , it may prompt the user for confirmation before executing such\nstate-changing actions. Finally, the annotator can also choose Ignore in case they select a wrong\nelement. Once all the actions have been annotated, the annotator can choose to complete the task.\nThey will then be asked to confirm the task description again and make any necessary modifications.\nPop-ups and CAPTCHAs. In this study, we emphasize on clean and direct task execution actions,\nintentionally omitting extraneous steps like pop-ups and CAPTCHAs that might introduce ambiguity\nin evaluation. We carefully select only those websites that pose no access issues when used with our\ntool. Before recording the task demonstration, annotators are requested to familiarize themselves with\nthe website, and preemptively close pop-up windows and clear CAPTCHAs to avoid their recurrence\nduring the actual demonstration. Annotators are further guided not to engage in extra steps such as\nclosing ads unless necessary during the task demonstration. In the final task verification, we revisit\nthe actions and filter out those unrelated to direct task execution. At the same time, we acknowledge\nthat these instances constitute a significant aspect of the dynamic web environment in the real world.\nEnhancing systems to robustly tackle such scenarios on-the-go could form an interesting avenue for\nfuture research.\nMitigating Disruptions on the Websites. The annotator are advised against actions that could\npotentially interfere the normal operation of the website. To handle tasks such as scheduling\nappointments, we introduce a Click (Fake) operation that annotators can utilize to indicate the\naction without actually executing it on the website.", "start_char_idx": 66976, "end_char_idx": 71855, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5203e67d-96a2-45d0-ad6f-09d31ee2e4ab": {"__data__": {"id_": "5203e67d-96a2-45d0-ad6f-09d31ee2e4ab", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e37ca8f1-f3e3-4f94-ba1c-ca5f8ce6dc59", "node_type": "1", "metadata": {}, "hash": "b177afa73cc06d3e0fbcefee96a21349569218daca2e45fb709560b6aa731034", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5744f482-e40e-49e1-9e90-dee0c9d702b6", "node_type": "1", "metadata": {}, "hash": "60c57540bcded5a91435c7a322e53eafcdcbe60b1633e45832a59f09db4ec521", "class_name": "RelatedNodeInfo"}}, "text": "Annotators are further guided not to engage in extra steps such as\nclosing ads unless necessary during the task demonstration. In the final task verification, we revisit\nthe actions and filter out those unrelated to direct task execution. At the same time, we acknowledge\nthat these instances constitute a significant aspect of the dynamic web environment in the real world.\nEnhancing systems to robustly tackle such scenarios on-the-go could form an interesting avenue for\nfuture research.\nMitigating Disruptions on the Websites. The annotator are advised against actions that could\npotentially interfere the normal operation of the website. To handle tasks such as scheduling\nappointments, we introduce a Click (Fake) operation that annotators can utilize to indicate the\naction without actually executing it on the website.\nB.4 Task Verification\nAll collected data undergoes an additional verification process conducted by the authors, as demon-\nstrated in Figure 11. The verification interface is shown in Figure 11. This verification consists of\nthree tasks. Firstly, we evaluate whether a task should be discarded due to its low quality. Secondly,\nwe examine each step to determine if any action should be discarded. This includes reviewing the\n19\n\nFigure 11: Illustration of our verification tool.\ninitial and final actions of the task, and excluding any additional actions (e.g., closing ads) that are\nnot outlined in the task description to ensure consistency across task annotations. Finally, we verify\nthe task description to confirm that all actions are accurately represented and make modifications if\nnecessary. If there is uncertainty regarding any action, the verifier can opt for the \u2018unsure\u2019 option,\nprompting a re-evaluation by the first author.\nC Experiment Details\nC.1 Evaluation\nOne complication that arises during evaluation on real-world websites is that multiple elements on a\nwebpage may induce the same effect. For instance, a button might house a text span within it, both\nof which, when clicked, yield identical results. To enhance the robustness of our evaluation, we\nemploy heuristics to detect elements equivalent to the ground truth. We first examine the ancestors\nof the labeled element to identify potential higher-level elements acceptable for the current action.\nWe employ a straightforward heuristic that locates the nearest clickable element to the ground\ntruth, including itself. After identifying the top-level acceptable element, we include all its visible\ndescendants that are located within its post-rendering bounding box as acceptable as well. Manual\nchecking on 100instances where the heuristic identifies a top-level element other than the ground\ntruth confirms the validity of the approach. For both training and evaluation stages, all acceptable\nelements are considered positive.\nC.2 Model Implementation Details\nCandidate Generation. We use the Cross-Encoder implementation from Sentence-Transformers4\nand use DeBERTa as the backbone model. More specifically, we use DeBERTa-v3-base5for our\nexperiments.\nAction Prediction. We use the Seq2Seq model implementation from Transformers [ 38]. We\nexperiment with the base6,large7andxl8versions of Flan-T5 [10].\n4https://www.sbert.net/examples/applications/cross-encoder/README.html\n5https://huggingface.co/microsoft/deberta-v3-base\n6https://huggingface.co/google/flan-t5-base\n7https://huggingface.co/google/flan-t5-large\n8https://huggingface.co/google/flan-t5-xl\n20\n\nTable 4: Hyperparameters used in experiments.\nMethod Model Hyperparamerters\nCandidate Generation deberta-v3-base batch_size: 32, epoch: 5,\nlearning_rate: 3e\u22125\nAction Prediction\nGeneration flan-t5-base batch_size: 32, epoch: 5,\nlearning_rate: 5e\u22125\nMINDACT flan-t5-base,\nflan-t5-large,\nflan-t5-xlbatch_size: 32, epoch: 5,\nlearning_rate: 5e\u22125\ngpt-3.5-turbo,\ngpt-4temperature: 0, # demonstrations: 3\nTable 5: Step Success Rate for Flan-T5 models with different groups of options. Here we shown\nmean and standard deviation of 5 runs with different random seeds.\nCross-Task Cross-Website Cross-Domain\nFlan-T5 B 41.5\u00b10.7 30 .0\u00b10.8 31 .3\u00b10.5\nFlan-T5 L 49.9\u00b10.2 35 .7\u00b10.5 36 .7\u00b10.3\nFlan-T5 XL 51.9\u00b10.8 39 .5\u00b10.2 39 .6\u00b10.2\nLLM In-context Learning.", "start_char_idx": 71029, "end_char_idx": 75228, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5744f482-e40e-49e1-9e90-dee0c9d702b6": {"__data__": {"id_": "5744f482-e40e-49e1-9e90-dee0c9d702b6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5203e67d-96a2-45d0-ad6f-09d31ee2e4ab", "node_type": "1", "metadata": {}, "hash": "0c0d5ea9730b3fc4c20acc78dfd9e5ceeb137a763ff329b1c0c702c6cb894d94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8c0879f-22ef-4b74-9485-fc84e0be0b5d", "node_type": "1", "metadata": {}, "hash": "8d530974cc8865dc44af4ba8a938f40013b8aadc1ade4e35901725c63926a567", "class_name": "RelatedNodeInfo"}}, "text": "Here we shown\nmean and standard deviation of 5 runs with different random seeds.\nCross-Task Cross-Website Cross-Domain\nFlan-T5 B 41.5\u00b10.7 30 .0\u00b10.8 31 .3\u00b10.5\nFlan-T5 L 49.9\u00b10.2 35 .7\u00b10.5 36 .7\u00b10.3\nFlan-T5 XL 51.9\u00b10.8 39 .5\u00b10.2 39 .6\u00b10.2\nLLM In-context Learning. We use the OpenAI API for in-context learning with LLMs. We\nexperiment with two versions of GPT models: gpt-3.5-turbo andgpt-4 . We include three\ndemonstration examples for in-context learning. The complete prompt is shown in Table 8.\nTraining Details. Theflan-t5-xl andflan-t5-large models are trained on servers with 4*A100\n80GB cards provided by Ohio Supercomputer Center [ 6]. All other models are trained with single\nA6000 48GB cards.\nPlease see Table 4 for all hyperparameters used in our experiments.\nD Additional Results\nD.1 Effect of Random Grouping Elements for Action Prediction\nFor both training and inference, we shuffle the elements in the webpage and randomly group them\ninto multi-choice questions. The model might give different predictions when presented with different\nsets of choices, and leads to slightly different final evaluation scores. Here we show the average and\nstandard deviation of 5runs with different random seeds to show the effect of random grouping. As\nwe can see from Table 5, the selection of choices only lead to small changes in overall performance\nwith standard deviation less than 1 for all runs.\nD.2 Zero-shot Results for Flan-T5 XL\nSince Flan-T5 is tuned with multi-choice format, it can also do element selection in zero-shot.\nHowever, as we can see from Table 6, while the model still gets some elements correct, it is much\nlower compared to the fine-tuned model, and 3-shot GPT 3.5/4. This is expected, since Flan-T5 is\nnot tuned for HTML and coding related tasks.\nTable 6: Zero-shot element selection results for Flan-T5 XLcompared with the fine-tuned counterpart.\nCross-Task Cross-Website Cross-Domain\nFlan-T5 XLZero-Shot 10.8 7 .8 11 .7\nFlan-T5 XLFine-Tuned 52.0 38 .9 39 .6\n21\n\nTable 7: Step Success Rate for all methods on the 50tasks subsets we used to evaluate GPT-4.\nNumbers in parentheses are the results on the full test set (same as Table 2)\nCross-Task Cross-Website Cross-Domain\nFlan-T5 B 43.3(41.0) 25.3(29.5) 28.1(31.6)\nFlan-T5 L 48.1(50.3) 30.8(35.3) 27.6(37.3)\nFlan-T5 XL 47.9(52.0) 33.3(38.9) 34.6(39.6)\nGPT-3.5 15.2(17.4) 15.1(16.2) 16.7(18.6)\nGPT-4 36.2 30 .1 26 .4\nD.3 Results on the 50task subsets\nDue to budget constraint, we only run GPT-4 on 50tasks for each setting. Here we show the step\nsuccess rate results for other methods on the same 50examples that GPT-4 is tested on, As we can\nsee from Table 7, the results on the 50tasks subsets are consistent with the results on the respective\nfull test set, and the relative performance across methods and splits remains the same.\n22\n\nTable 8: Prompt for action prediction in MINDACTwith GPT models.\nOnly part of the HTML snippet is shown here to save space.\nRole Content\nsystem You are a helpful assistant that is great at website design, navigation, and executing\ntasks for the user\nuser ```\n<html> <div> <div> <a tock home page /> <button id=0 book a reservation. toggle\nopen> <span> Book a reservation </span> </button> <button book a reservation.", "start_char_idx": 74967, "end_char_idx": 78198, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8c0879f-22ef-4b74-9485-fc84e0be0b5d": {"__data__": {"id_": "e8c0879f-22ef-4b74-9485-fc84e0be0b5d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5744f482-e40e-49e1-9e90-dee0c9d702b6", "node_type": "1", "metadata": {}, "hash": "60c57540bcded5a91435c7a322e53eafcdcbe60b1633e45832a59f09db4ec521", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d05689a-2158-495f-a8fb-050b7cb346ed", "node_type": "1", "metadata": {}, "hash": "7bb349382e750065d4dd1709d9208e1eca63e2f5f6237aa9201ca2911a0c9c54", "class_name": "RelatedNodeInfo"}}, "text": "Here we show the step\nsuccess rate results for other methods on the same 50examples that GPT-4 is tested on, As we can\nsee from Table 7, the results on the 50tasks subsets are consistent with the results on the respective\nfull test set, and the relative performance across methods and splits remains the same.\n22\n\nTable 8: Prompt for action prediction in MINDACTwith GPT models.\nOnly part of the HTML snippet is shown here to save space.\nRole Content\nsystem You are a helpful assistant that is great at website design, navigation, and executing\ntasks for the user\nuser ```\n<html> <div> <div> <a tock home page /> <button id=0 book a reservation. toggle\nopen> <span> Book a reservation </span> </button> <button book a reservation.\ntoggle open> </button> </div> <div> <select id=1 type> <option reservations true>\nDine in </option> ... </html>\n```\nBased on the HTML webpage above, try to complete the following task:\nTask: Check for pickup restaurant available in Boston, NY on March 18, 5pm with\njust one guest\nPrevious actions:\nNone\nWhat should be the next action? Please select from the following choices (If the\ncorrect action is not in the page above, please select A. \u2018None of the above\u2019):\nA. None of the above\nB. <button id=0 book a reservation. toggle open> <span> Book a\nC. <select id=1 type> <option reservations true> Dine in </option> <option\nD. <div id=2> <p> Celebrating and supporting leading women shaking up\nassistant Answer: C.\nAction: SELECT\nValue: Pickup\nuser ```\n<html> <div> <main main> <section tabpanel> <div> <ul tablist> <li tab heading\nlevel 3 search and> </li> <li id=0 tab heading level 3 search and> <span> Hotel\n</span> </li> <li tab heading level 3 search and> </li> <li tab heading level 3\nsearch and> </li> </ul> <div tabpanel> <div id=1> <div> <span> Dates* </span>\n<button button clear dates /> </div> ... </html>\n```\nBased on the HTML webpage above, try to complete the following task:\nTask: Compare the fare types to book a 1-adult ticket from Springfiels, IL to Austin,\nTX for April 29th 2023\nPrevious actions:\n[combobox] Enter your departing city, airport name, or airpor... -> TYPE:\nSPRINGFIELD\n[button] Springfield, IL, US (SPI) -> CLICK\n[combobox] Enter your destination city, airport name, or airp... -> TYPE: AUSTIN\n[button] Austin, TX, US (AUS) -> CLICK\nWhat should be the next action? Please select from the following choices (If the\ncorrect action is not in the page above, please select A. \u2018None of the above\u2019):\nA. None of the above\nB. <li id=0 tab heading level 3 search and> <span> Hotel\nC. <div id=1> <div> <span> Dates* </span> <button button clear dates\nD. <ul id=2> <a mobile tools> </a> <a open united\u2019s tiktok\nassistant Answer: A.\nContinued on next page\n23\n\nTable 8 \u2013 continued from previous page\nRole Content\nuser ```\n<html> <div> <nav main menu> <ul> <li> <div button> Car Sales </div> <div\nid=0> <div> <div> <div> Buy A Car </div> <div> Plan Your Purchase </div>\n</div> <div> <h4> Its Tax Refund Time. Treat Yourself to an Upgrade. </h4> <p>\nWith a variety of options, invest your refund in what you really want - a quality,\nused vehicle from Enterprise. </p> ... </html>\n```\nBased on the HTML webpage above, try to complete the following task:\nTask: Find a mini van at Brooklyn City from April 5th to April 8th for a 22 year\nold renter.\nPrevious actions:\n[searchbox] Pick-up & Return Location (ZIP, City or Airport) (... -> TYPE:\nBrooklyn\n[option] Brooklyn, NY , US Select -> CLICK\nWhat should be the next action? Please select from the following choices (If the\ncorrect action is not in the page above, please select A.", "start_char_idx": 77468, "end_char_idx": 81048, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d05689a-2158-495f-a8fb-050b7cb346ed": {"__data__": {"id_": "1d05689a-2158-495f-a8fb-050b7cb346ed", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8141fb3-ab68-474a-9a84-2b5e334b1978", "node_type": "4", "metadata": {}, "hash": "965054b46f333a2dc6c2e8b84ba534ba9b773820373e22f855aa37ef0cc05669", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8c0879f-22ef-4b74-9485-fc84e0be0b5d", "node_type": "1", "metadata": {}, "hash": "8d530974cc8865dc44af4ba8a938f40013b8aadc1ade4e35901725c63926a567", "class_name": "RelatedNodeInfo"}}, "text": "Treat Yourself to an Upgrade. </h4> <p>\nWith a variety of options, invest your refund in what you really want - a quality,\nused vehicle from Enterprise. </p> ... </html>\n```\nBased on the HTML webpage above, try to complete the following task:\nTask: Find a mini van at Brooklyn City from April 5th to April 8th for a 22 year\nold renter.\nPrevious actions:\n[searchbox] Pick-up & Return Location (ZIP, City or Airport) (... -> TYPE:\nBrooklyn\n[option] Brooklyn, NY , US Select -> CLICK\nWhat should be the next action? Please select from the following choices (If the\ncorrect action is not in the page above, please select A. \u2018None of the above\u2019):\nA. None of the above\nB. <div id=0> <div> <div> <div> Buy A Car </div> <div>\nC. <div id=1> Enterprise Fleet Management </div>\nD. <button id=2 selected pick-up date 03/19/2023> <span> <span> 19 </span>\nassistant Answer: D.\nAction: CLICK\n24", "start_char_idx": 80429, "end_char_idx": 81308, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"d8141fb3-ab68-474a-9a84-2b5e334b1978": {"node_ids": ["16795457-88d0-43b4-a2a0-816bd0398062", "16863c14-7fa4-40f2-9675-f6d1f5e73b0f", "ed2edda2-6d00-4d88-b6ba-b3c38f530584", "de15932a-26f7-4142-87a9-4438ba6eeae6", "f9c1e995-6d7d-43ba-a1ba-c5344007638d", "26f4fed3-48f4-43bf-b791-3eb09d24ed5f", "19d730d0-825a-4439-885a-dbd50d1bc147", "b49ce16f-348c-4cd1-8cb6-9dcc74e7ae4c", "e544e3f6-6ab6-4f5d-a2b2-b98da9651da5", "a9afb300-8619-430f-9ff8-06ed71a402be", "b261699b-4d9a-4888-888b-696381fca41a", "6580de09-1c7e-4d29-87d2-f4a62f8b9f05", "844cd9dc-85f5-4e67-8617-80481d075dc9", "5a5dea97-9e72-4d47-851d-51d7d912346a", "be73ca2b-08ae-4ae5-a08b-5f503f529c49", "f93a121e-470a-40bb-abb9-967d52be7211", "0aee29e8-1b66-41f9-ac12-9d47a48e9455", "31ec87f2-0d33-4194-922f-c094e260481a", "9fd5bcc5-308b-4b1e-94f7-a3c19d73d366", "12955511-a656-4d76-b211-062c0c679438", "e3f4f903-820e-4236-8012-b6418af40470", "e37ca8f1-f3e3-4f94-ba1c-ca5f8ce6dc59", "5203e67d-96a2-45d0-ad6f-09d31ee2e4ab", "5744f482-e40e-49e1-9e90-dee0c9d702b6", "e8c0879f-22ef-4b74-9485-fc84e0be0b5d", "1d05689a-2158-495f-a8fb-050b7cb346ed"], "metadata": {}}}}